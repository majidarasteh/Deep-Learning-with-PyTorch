# Classification Models Using Pytorch
**Classification** is a type of supervised machine learning task where the goal is to predict a categorical label or class for a given input. Instead of predicting a continuous numerical value (like in regression), classification assigns data points to predefined categories or groups.

Binary Classification is a fundamental type of machine learning task where the goal is to categorize data into one of two distinct groups or classes. Think of it as a simple "either/or" decision. The model learns from historical data to find patterns that distinguish the two categories, and when presented with new, unseen data, it predicts which of the two labels is most likely. In essence, it's about making a clear-cut choice between two possible outcomes.

* **Real-world examples**: spam detection, medical diagnosis, fraud detection
* **Mathematical formulation**: `f(x) → {0, 1} or {-1, 1}`

At its core, **classification is about pattern recognition and decision-making**. You show the computer many examples that are already labeled, like emails marked as "spam" or "not spam", and it figures out what patterns distinguish each category. Once trained, when you show it a new email, it can decide which category it most likely belongs to based on those learned patterns.

There are different types of classification depending on how many categories you need. **Binary classification** involves just two options, like "yes/no" decisions—is this tumor cancerous or not? **Multi-class classification** handles several categories, like identifying whether an image shows a cat, dog, or bird. The principles remain the same, but the complexity increases with more categories.

**Real-world Examples of Binary classification**

| Application              | Input Features                             | Output Classes  
| -------------------------| ------------------------------------------ | --------------------
| **Email Filtering**      | Email content, sender, subject             | Spam / Not Spam             
| **Medical Diagnosis**    | Symptoms, test results, patient history    | Diseased / Healthy                
| **Image Recognition**    | Pixel values, shapes, colors               | Cat / Dog        
| **Loan Approval**        | Income, credit score, employment           | Approve / Reject 


**Multi-class classification** is a type of classification task where there are more than two categories to predict. Unlike binary classification (which answers "yes/no" or "A/B" questions), multi-class classification answers "which one" questions with multiple possible answers. Key characteristics:
1. **Mutually Exclusive Classes**: Each sample belongs to exactly one category. For examples, animal species (can't be both cat and dog).
2. **Multiple Decision Boundaries**: Instead of one line separating two classes, you need boundaries between all class pairs.

**Probability Distribution**

For example, for three classes, outputs probabilities are:
* Class A: 0.75
* Class B: 0.20
* Class C: 0.05
  
Final prediction: Class with highest probability. Real-world examples:

| Domain                   | Binary Classification                      | Multi-class Classification 
| -------------------------| ------------------------------------------ | --------------------
| **Email**                | Spam vs Not Spam                           | Work/Personal/Social/News/Spam           
| **Medical**              | Cancer vs No Cancer                        | Cold/Flu/COVID/Allergies/Healthy          
| **Image**                | Cat vs Dog                                 | Cat/Dog/Bird/Horse/Fish         
| **Products**             | Defective vs OK                            | Electronics/Clothing/Books/Home/Food

**Multi-class classification** is essential because most real-world classification problems naturally have more than two categories. The choice of method depends on your specific problem, data characteristics, and computational resources.

## Fundamental Workflow for Building any Neural Network
Those four steps represent the fundamental workflow for building any neural network model, not just for binary classification. 

### Step 1. Build the Model
This is where you define the architecture of the neural network (its brain). In this step, you specify the layers, the number of neurons in each layer, and the activation functions. Here is a pytorch code example:

```python
import torch.nn as nn

class BinaryClassifier(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.layer1 = nn.Linear(input_size, 10)  # Hidden layer
        self.relu = nn.ReLU()
        self.output_layer = nn.Linear(10, 1)     # Output layer
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.sigmoid(self.output_layer(x))
        return x

model = BinaryClassifier(input_size=8)  # Create the model instance
```

### Step 2. Compile the Model
This step configures the model for training. You define the "rules of learning". In this step, you choose three key components:
1. **Optimizer**: The algorithm that updates the model's internal parameters (weights) to reduce error (e.g., `Adam`, `SGD`).
2. **Loss Function**: The function that measures how wrong the model's predictions are. For binary classification, this is almost always `BCELoss` (**Binary Cross-Entropy**) or the more stable `BCEWithLogitsLoss`.
3. **Metrics (Optional but important)**: The human-readable measures you want to track, like accuracy.

**PyTorch Code Example:**

```python
# In PyTorch, "compiling" is done by defining these separately
criterion = nn.BCELoss()           # Loss Function
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Optimizer
# Metrics (like accuracy) are calculated manually during training/evaluation
```

### Step 3. Fit the Model (Train the Model)
This is the actual training process where the model learns from the data. In this step, you feed the training data (features `X_train` and labels `y_train`) to the model. The model makes predictions, calculates the loss, and then the optimizer adjusts the model's weights to make better predictions next time. This is repeated for a set number of epochs (iterations over the entire dataset). PyTorch code example (training loop):

```python
for epoch in range(100):  # Number of epochs
    # Forward pass: Make prediction
    y_pred = model(X_train)
    loss = criterion(y_pred, y_train)

    # Backward pass: Learn from mistakes
    optimizer.zero_grad()  # Clear previous gradients
    loss.backward()        # Calculate gradients
    optimizer.step()       # Update weights
```

### Step 4. Evaluate the Model
This is the test. You check how well your trained model performs on new, unseen data to see if it has learned general patterns or just memorized the training set. In this step, you use the model to make predictions on the test set (`X_test`). You then compare these predictions to the true test labels (`y_test`) that the model has never seen before. You use **metrics like accuracy, precision, and recall** to judge its performance. Here is a pyTorch code example:

```python
model.eval()  # Set the model to evaluation mode
with torch.no_grad():  # Turn off gradient calculation for efficiency
    test_predictions = model(X_test)
    test_predictions = (test_predictions > 0.5).float()  # Convert probabilities to 0 or 1
    accuracy = (test_predictions == y_test).float().mean()
print(f'Test Accuracy: {accuracy:.4f}')
```

This four-step process provides a clean, logical framework for going from an idea to a functioning machine learning model:
* **Build**: Design the architecture (brain).
* **Compile**: Set the learning rules.
* **Fit**: Learn from data.
* **Evaluate**: Test on new data.

## Evaluation Metrics
1. **Accuracy**

   Accuracy measures the proportion of correctly predicted samples out of all samples. It gives a general idea of how often the model is correct. Accuracy is simple and useful but may be **misleading in imbalanced datasets**, where one class dominates.

    <img width="314" height="56" alt="image" src="https://github.com/user-attachments/assets/4ce795d3-6a5c-406d-8ea0-5d8b29aaeec3" />

    * **TP**: True Positives (correctly predicted positive samples)
    * **TN**: True Negatives (correctly predicted negative samples)
    * **FP**: False Positives (incorrectly predicted as positive)
    * **FN**: False Negatives (incorrectly predicted as negative)
      
2. **Precision**
   
   Precision measures how many of the predicted positive samples are actually positive. It shows the quality of positive predictions. A high precision means that when the model predicts a positive, it’s usually correct. Useful when false positives are costly (e.g., spam detection).

   <img width="202" height="63" alt="image" src="https://github.com/user-attachments/assets/3cc21ad9-3084-4544-b944-a37134e0d0c8" />

3. **Recall (Sensitivity or True Positive Rate)**
   
   Recall measures how many of the actual positive samples the model correctly identified. It shows the model’s ability to capture positive instances. A high recall means the model misses very few positive cases. Important when false negatives are costly (e.g., disease detection).

   <img width="181" height="60" alt="image" src="https://github.com/user-attachments/assets/5de6acac-d922-448e-861c-682ea61440c7" />

4. **F1-Score**

   The F1-score is the harmonic mean of Precision and Recall. It provides a balance between the two metrics, especially useful when data is imbalanced. A high F1-score means the model performs well in both identifying positives and minimizing false alarms.

   <img width="328" height="68" alt="image" src="https://github.com/user-attachments/assets/8787d647-0ba8-4de4-978e-29547ef8fc24" />

5. **Confusion Matrix**

   A confusion matrix is a summary table showing the counts of correct and incorrect predictions for each class. It helps visualize how well the model distinguishes between classes. This matrix helps identify where the model makes specific types of mistakes. For binary classification:

|                     | Predicted Positive  | Predicted Negative  |
| ------------------- | ------------------- | ------------------- |
| **Actual Positive** | True Positive (TP)  | False Negative (FN) |
| **Actual Negative** | False Positive (FP) | True Negative (TN)  |


```python
from sklearn.metrics import classification_report

y_true_np = y_true.cpu().numpy()
y_pred_np = y_pred.cpu().numpy()
print(confusion_matrix(y_true_np, y_pred_np))
```

6. **Classification Report**

   The classification report provides a detailed summary of Precision, Recall, F1-score, and support (number of true instances per class) for each class. It is especially helpful for multi-class classification problems. In Python (using scikit-learn), you can generate it with:

```python
from sklearn.metrics import classification_report

y_true_np = y_true.cpu().numpy()
y_pred_np = y_pred.cpu().numpy()
print(classification_report(y_true_np, y_pred_np))
```

**In summary:**
* **Accuracy** → Overall correctness
* **Precision** → Reliability of positive predictions
* **Recall** → Ability to capture actual positives
* **F1-score** → Balance between precision and recall
* **Confusion Matrix** → Detailed performance visualization
* **Classification Report** → Compact summary for all metrics per class

## Tips
1. When designing a binary classification model in PyTorch, it’s important to match your output layer with the correct loss function.
   * If you include a `nn.Sigmoid()` activation in your model’s output layer, you should use `nn.BCELoss`, since this loss function expects probability values between 0 and 1.
   * However, if you don’t use `sigmoid()` in your output layer and instead output raw logits, you should use `nn.BCEWithLogitsLoss`, which internally applies the sigmoid operation in a numerically stable way. This approach helps prevent vanishing gradients and improves training stability.

2. Implementing a neural network is an **experimental and iterative process**. You usually start by building a base model with a simple architecture, then gradually **tune its layers, number of neurons, activation functions, and hyperparameters** such as learning rate or batch size. The goal is to find a balance where the model **learns effectively without underfitting or overfitting**. Continuous experimentation, validation, and performance monitoring are key steps toward achieving a well-generalized and robust model.

3. For **multiclass classification** tasks, always use `CrossEntropyLoss` instead of `BCELoss` or `BCEWithLogitsLoss`. The `CrossEntropyLoss` function is designed for problems where each sample belongs to exactly one class, taking raw logits as input and integer class labels as targets.

4. In **multiclass classification** tasks, you should use `CrossEntropyLoss`, which is specifically designed for cases where each input belongs to exactly one of several classes. The model’s output should have the shape `[batch_size, num_classes]`, representing the raw scores (logits) for each class. The target tensor must be one-dimensional with shape `[batch_size]`, containing integer class labels ranging from 0 to $num_classes - 1$. It’s important not to reshape the target to `[batch_size, 1]`, as CrossEntropyLoss automatically handles the correspondence between logits and class indices.

5. In **multiclass classification**, always set the correct data types for your tensors: use `dtype=torch.float32` for the input features and `dtype=torch.long` for the target labels. The targets must be integers representing class indices, as required by `CrossEntropyLoss` in PyTorch.
