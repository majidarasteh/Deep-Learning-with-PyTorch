# Introduction to Regression with PyTorch
**Regression** is a fundamental type of supervised machine learning problem where the goal is to predict a **continuous-valued output** based on a set of input features.

In other words, regression models aim to estimate the **relationship between dependent variables** (the outputs/targets we want to predict) and **independent variables** (the input features we use for the prediction).

**Key Characteristic:** The output is not a category (like "cat" or "dog") but a **numerical value on a continuous scale**. Examples include predicting a person's salary, the temperature tomorrow, or the selling price of a house.

### A Practical Example - Predicting House Prices
Imagine we are a real estate company, and we want to build a model that can predict the market price of a house.
1. **Our Goal**: Predict the price of a house (the continuous output).
2. **Our Inputs (Features):** We use several properties of the house as our inputs:
   * Number of Bedrooms
   * Number of Bathrooms
   * Number of Parking Spaces
   * Square Footage
   * Year Built
  
  ##  Building a PyTorch Model
  Building a model in PyTorch follows a systematic workflow. 

  1. **Step 1: Model Construction (Building the Architecture)**
     
     In this step, we define the **structure of our neural network** by specifying its input, hidden, and output layers.
     * **Input Layer:** This is determined by the **number of features in your data**. For our house price example, if we use 5 features (bedrooms, bathrooms, etc.), the input layer must have 5 neurons.
     * **Hidden Layer(s):** These are the **intermediate layers** where the model learns complex, non-linear relationships between the inputs and the output. We can have multiple hidden layers with any number of neurons. This is where the "magic" of learning happens.
     * **Output Layer:** For a **regression** task, the output layer typically has a **single neuron** that produces the **predicted continuous value** (e.g., the house price).

   In **PyTorch**, we build this by **creating a class** that inherits from `nn.Module` and **defining the layers** in the ```__init__``` method and the data flow in the forward method.

   ```python
  import torch.nn as nn
  
  class RegressionModel(nn.Module):
      def __init__(self, input_size):
          super(RegressionModel, self).__init__()
          self.layer1 = nn.Linear(input_size, 50) # Input to hidden layer
          self.layer2 = nn.Linear(50, 1)          # Hidden to output layer
  
      def forward(self, x):
          x = torch.relu(self.layer1(x))
          x = self.layer2(x)
          return x
   ```
     
  2. **Step 2: Model Compilation (Configuring the Learning Process)**
     
     This step involves setting up the components that control how the model learns. It consists of three key parts:
     * **Loss Function (The Measure of Error):** This function calculates how far the model's predictions are from the true values. The goal of training is to minimize this loss. For regression, common choices are:
       - `nn.MSELoss()` (Mean Squared Error): Good all-purpose choice, heavily penalizes large errors.
       - `nn.L1Loss()` (Mean Absolute Error): Less sensitive to outliers.
     * **Optimizer (The Learning Algorithm):** The optimizer is responsible for **updating the model's parameters** (weights and biases) to reduce the loss. It does this **using a method called gradient descent**. Popular optimizers include:
       - `torch.optim.SGD` (Stochastic Gradient Descent)
       - `torch.optim.Adam` (Adaptive Moment Estimation - very popular and often works well)
     * **Evaluation Metrics (Interpreting Performance)**: Help the developers understand how well the model is performing. For regression, common metrics are:
       - `Mean Absolute Error (MAE)`: Easy to interpret (average error in price units).
       - `R-squared (R²)`: Explains how much of the variance in the target is explained by the model.
         
  ``` python
      model = RegressionModel(input_size=5)
      criterion = nn.MSELoss()       # Loss Function
      optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # Optimizer
  ```

  3. **Step 3: Model Fitting (Training the Model)**
     
     This is the core process where we **allow the model to find patterns** between the inputs and outputs. The training loop involves the following steps for each batch of data:
     * **Forward Pass:** Pass the input data through the model to get a prediction.
     * **Calculate Loss:** Compare the prediction against the true value using the loss function.
     * **Backward Pass:** Calculate the gradients (derivatives) of the loss with respect to every model parameter. This tells us how to adjust the parameters to reduce the error.
     * **Update Parameters:** The optimizer uses these gradients to take a "step," updating the weights and biases.

  ``` python
      for epoch in range(num_epochs):
          # Forward pass
          predictions = model(X_train)
          loss = criterion(predictions, y_train)
      
          # Backward pass and optimize
          optimizer.zero_grad() # Clear old gradients
          loss.backward()       # Calculate new gradients
          optimizer.step()      # Update parameters
  ```

  4. **Step 4: Model Evaluation (Testing on Unseen Data)**
     
     After training, we must evaluate the model on a **test dataset** that it has **never seen before**. The goal of this evaluation is to answer a critical question: How reliable is our model?
     * **Overfitting**: A model that performs well on training data but poorly on test data. The model has memorized the training examples instead of learning generalizable patterns.
     * **Process**: We run the trained model on the test set and calculate our evaluation metrics (e.g., **MAE, R²**) on these new predictions.
     * **Outcome**: A low error on the test set gives us confidence that the model will perform reliably in the real world.

  ``` python
        with torch.no_grad(): # Disable gradient tracking for efficiency
            test_predictions = model(X_test)
            test_loss = criterion(test_predictions, y_test)
            print(f"Test Loss (MSE): {test_loss:.4f}")
  ```

## Step 1: Creating the Neural Network Model
We **architect the neural network** by defining its structure. The choices made here fundamentally impact the model's capacity to learn complex patterns. The three main considerations are:
1. **A**) Determining the number of hidden layers
2. **B**) Determining the number of neurons in each layer
3. **C**) Selecting appropriate activation functions for each layer

### A) Determining the Number of Hidden Layers
The depth of the network (number of hidden layers) determines what kind of relationships it can learn.
* **No Hidden Layer** (Linear Regression): Can only learn linear relationships.
* **1-2 Hidden Layers**: Can learn non-linear, complex patterns. **Sufficient for most common regression tasks** (like house price prediction).
* **Many Hidden Layers (Deep Network)**: Can learn very complex, hierarchical patterns. Risk of overfitting if the dataset is not large enough.
  
**Recommendation**: Start with 1-2 hidden layers for regression problems and increase complexity if needed.

### B) Determining the Number of Neurons in Each Layer
The **width of each layer** (number of neurons) controls the "learning capacity" of that layer.
* **Input Layer Neurons**: Must match the number of features in your dataset.
* **Hidden Layer Neurons**: There's no perfect formula, but common practices are:
  - A number between the input and output layer sizes.
  - Using powers of 2 for computational efficiency (e.g., 64, 128, 256).
  - Start with a moderate number (e.g., 50-100) and adjust based on performance.
* **Output Layer Neurons:** For standard **regression** (predicting a single value), **this is always 1**.

### C) Selecting Appropriate Activation Functions
Activation functions introduce **non-linearity** into the network. Without them, the entire network would be just a linear model, no matter how many layers.

**Common activation functions for hidden layers:**
* **ReLU (Rectified Linear Unit)**: `f(x) = max(0, x)`
* **Leaky ReLU:** A variant that allows a small, non-zero gradient when the unit is inactive.
* **Tanh (Hyperbolic Tangent)**: `f(x) = tanh(x)`

**Output Layer Activation:**
* **For regression**, the output layer **typically uses no activation function** (linear activation) or `nn.Identity()`, as we want to predict any continuous value without constraints.


## Hyperparameter Tuning in Neural Networks
The optimal configuration often emerges through iterative experimentation, validation, and refinement based on performance metrics and domain knowledge. This comprehensive approach to hyperparameter tuning ensures that neural networks not only achieve high performance on training data but also maintain robust generalization capabilities when deployed in real-world applications.

**Hyperparameter tuning** is a crucial process in machine learning that significantly depends on the **practitioner's experience** and intuition. While there are systematic approaches, the ability to effectively tune models often comes from hands-on practice and learning from previous experiments. Throughout this learning process, we must remain vigilant about **overfitting** - when a model performs well on training data but fails to generalize to unseen data. Four key areas for model improvement:
1. **Model Architecture Design**
   * This involves strategically **adding hidden layers** and determining the **optimal number of neurons in each layer**.
   * More complex architectures with numerous layers and neurons can capture intricate patterns but **risk overfitting**. 
   * The **choice of activation functions** for each layer is equally critical, as different functions like ReLU, Tanh, or Sigmoid can significantly impact the model's learning capability and convergence.
     
2. **Model Compilation Configuration**
   * During the compilation phase, we focus on selecting the **appropriate optimizer algorithm** and tuning its parameters, **particularly the learning rate**.
   * The **optimizer** determines **how the model updates its weights** during training, while the learning rate controls the size of these updates.
   * Finding the right balance is essential - too high a learning rate may cause the model to overshoot optimal solutions, while too low a rate can lead to extremely slow convergence or getting stuck in local minima.
  
3. **Training Process Optimization**
   * The **fitting** phase involves determining the **optimal number of training epochs**. Allowing the model to train for more epochs gives it additional time to learn complex patterns and refine its parameters.
   * This must be balanced with careful monitoring to **prevent overfitting**. Techniques like **early stopping** can help terminate training once performance on validation data stops improving, ensuring efficient use of computational resources while maintaining model generalization.
   * Longer training with more epochs can lead to **better convergence** but may cause the model to **memorize training data** rather than learning generalizable patterns.

4. **Data Strategy and Quantity**
   * The **quantity and quality of training data** fundamentally impact model performance.
   * **Using more diverse training data** helps the model learn robust features that generalize better to real-world scenarios.
   * In cases where collecting additional data is challenging, **data augmentation techniques** can artificially expand the dataset by creating modified versions of existing samples.
   * More data typically leads to better performance.
