# Introduction to Pytorch

PyTorch is an open-source, **Python-based deep learning library** widely used for both research and production. According to Papers With Code, PyTorch has been the most widely used deep learning framework in research since 2019. Furthermore, the Kaggle Data Science and Machine Learning Survey 2022 reported that around $40%$ of respondents use PyTorch and this rate continues to grow each year. Reasons why PyTorch is popular:
1. **User-friendly interface**: Easy to learn and use
2. **Efficiency**: Optimized performance
3. **Flexibility**: Allows low-level customization while maintaining accessibility
4. **Balanced design**: Perfect blend of usability and advanced features

### The Three Core Components
PyTorch can be understood in terms of its three main components.
1. **Tensor Library**
   * **Foundation**: Extends NumPy's array-oriented programming.
   * **Key advantage**: Seamless GPU acceleration while maintaining CPU compatibility.
   * **Purpose**: Fundamental building block for all computations.
     
2. **Automatic Differentiation (autograd)**
   * **Function**: Automatically computes gradients for tensor operations.
   * **Benefit**: Simplifies backpropagation and model optimization.
   * **Impact**: Eliminates manual gradient calculation, making neural network training much easier.

3. **Deep Learning Library**
   * **Features**: Modular building blocks, pretrained models, loss functions, optimizers.
   * **Design philosophy**: Flexible and efficient components.
   * **Audience**: Caters to both researchers (flexibility) and developers (ease of use)

## Artificial Intelligence (AI)
PyTorch is a deep learning library used to build and train AI models such as LLM and CNN. For instance, Large Language Models (LLMs) are often referred to as AI models. However, LLMs are also a type of deep neural network, and PyTorch is a deep learning library used to build and train such models. This overlap in terminology can be confusing, so let’s briefly summarize the relationship between these terms before proceeding (the AI hierarchy):

1. **Artificial Intelligence (AI)**
   * Systems that perform tasks requiring human intelligence.
   * **Examples**: Natural language understanding, pattern recognition, decision making.
     
2. **Machine Learning (ML) - Subset of AI**
   * Algorithms that learn from data without explicit programming.
   * Improve performance over time with more data and feedback.
   * Real-world applications:
     - Recommendation systems (Netflix, Amazon)
     - Email spam filtering
     - Voice recognition (Siri, Alexa)
     - Self-driving cars
       
3. **Deep Learning (DL) - Subset of Machine Learning**
   * Deep neural networks with multiple hidden layers
   * Loosely modeled after the human brain's neural connections
   * Excellent at handling unstructured data (images, audio, text)
   * Multiple layers enabling complex, nonlinear relationships
   * Large Language Models (LLM) are a type of Deep Neural Network, which makes them part of this hierarchy.

    <img width="313" height="314" alt="image" src="https://github.com/user-attachments/assets/a05dfa4a-754e-455a-a1fc-738cf449bfaf" />


 ### Supervised and Unsupervised Learning
 In **machine learning**, algorithms are often categorized based on the type of data available during training and the nature of the learning objective. Two of the most common paradigms are **supervised learning** and **unsupervised learning**. Each serves different purposes and is suited for different types of tasks.

1. **Supervised Learning**
   * Supervised learning involves training a model on a dataset that contains both **inputs (features)** and their corresponding **outputs (labels)**.
   * The model learns to map inputs to outputs by minimizing the difference between its predictions and the true labels.
   * **Common Tasks:**
     - **Classification**: Predict discrete categories (spam/not spam, cat/dog)
     - **Regression**: Predict continuous values (house prices, temperature)
       
   * **Example**
     - In an email spam classifier, each training sample (email) has a label—either “spam” or “not spam.”
     - LLM pretraining via next-word prediction (sequence → next word)
     - In image classification, the input might be an image, and the label could be a class such as “cat”, “dog”, or “car.”
       
   * **Common algorithms and models include:**
     - Linear regression and logistic regression
     - Decision trees and random forests
     - Support vector machines (SVMs)
     - Neural networks and deep learning architectures (e.g., CNNs, LSTMs, Transformers)

2. **Unsupervised Learning**
   * Unsupervised learning deals with data that does not contain labels.
   * The algorithm explores the structure of the data on its own to find patterns, relationships, or groupings among examples.
   * The goal is to uncover hidden structure rather than predict explicit outcomes.
   * Unlike supervised learning, where feedback is provided in the form of correct labels, unsupervised learning relies on data similarity and statistical relationships to discover structure.
     
   * **Common Tasks:**
     - **Clustering**: Group similar data points (customer segmentation)
     - **Anomaly Detection**: Find unusual data points
     - **Association: Discover** relationships between variables
       
   * **Examples:**
     - Grouping news articles by topic without knowing categories
     - Finding similar customers based on purchasing behavior
     - Reducing image dimensions while preserving important features
       
   * **Common algorithms**
     - K-means, hierarchical clustering, DBSCAN, ...

While supervised and unsupervised learning represent two ends of the machine learning spectrum, many real-world problems **fall somewhere in between**. In practice, **labeled data is often scarce** or expensive to obtain, while unlabeled data is abundant. This challenge has led to the development of **semi-supervised** and **self-supervised learning** techniques, both of which aim to make better use of available unlabeled data.

3. **Semi-supervised Learning**
   * Semi-supervised learning combines aspects of both supervised and unsupervised learning.
   * It leverages a small amount of labeled data together with a large amount of unlabeled data to improve model performance.
   * The key idea is that the model first learns the general structure or distribution of the data from the unlabeled portion and then refines its understanding using the limited labeled examples.
     
    * **Real-world Example:**
      - Medical imaging: 100 labeled tumor scans + 10,000 unlabeled scans
      - Text classification: 1,000 labeled customer reviews + 1 million unlabeled reviews
    * Semi-supervised learning has proven particularly effective when obtaining labeled data is costly or time-consuming, such as in healthcare, fraud detection, and natural language processing (NLP).
    * **Common techniques**
      - Self-training: Model labels its own predictions on unlabeled data, then retrains
      - Co-training: Multiple models teach each other using different data views

4. **Self-supervised Learning**
   * **Core Idea**: Generate labels automatically from the data itself.
   * **No human labels**: The data provides its own supervision signal
   * **Examples:**
     - Predict missing words in sentences
     - Next sentence prediction
     - Recover missing parts of images



| Learning Type       | Data Requirement                       | Uses Labels          | Example Tasks                        | Typical Use Case                        |
| ------------------- | -------------------------------------- | -------------------- | ------------------------------------ | --------------------------------------- |
| **Supervised**      | Labeled data only                      | ✅ Yes                | Classification, regression           | Spam detection, object recognition      |
| **Unsupervised**    | Unlabeled data only                    | ❌ No                 | Clustering, dimensionality reduction | Customer segmentation, topic modeling   |
| **Semi-supervised** | Mix of labeled + unlabeled             | ⚙️ Partially         | Classification with limited labels   | Medical imaging, fraud detection        |
| **Self-supervised** | Unlabeled data (auto-generated labels) | ⚙️ Derived from data | Representation learning, pretraining | LLMs, vision transformers, autoencoders |

## Installing PyTorch
**PyTorch** is a large, flexible deep learning library with both **CPU** and **GPU** support. Because of that, installation depends on your system’s hardware and Python environment.

1. **Python Version Compatibility**
   * Use a stable version that’s one or two releases behind the newest one.
   * For example, if the latest is Python 3.13, install Python 3.11 or 3.12 for best compatibility.
   * Reason: Scientific libraries take time to support newest Python versions
     
  2. **Basic CPU-only installation**
     * If you don’t have a GPU or don’t need GPU acceleration:

       ```
          pip install torch
       ```
     * Installing with **CUDA for NVIDIA GPUs
        - Go to https://pytorch.org
        - Use the installation selector to get the correct command for your OS and CUDA version.
        - Example (for PyTorch 2.4.0):

       ```
          pip install torch==2.4.0 torchvision torchaudio
       ```
  3. **Verify installation**
       * Run this in Python:
    
       ```
          import torch
          print(torch.__version__)
       ```
       * Expected output:
    
       ```
          '2.4.0'
       ```
       
4. **Check GPU availability**
   * To verify GPU support:

       ```
          import torch
          print(torch.cuda.is_available())
       ```
   * If it prints True, GPU acceleration is active.
   * If it prints False, check your GPU drivers, CUDA, or reinstall with the right CUDA version.
  
5. **Apple Silicon (M1/M2/M3) acceleration**
   * If using a Mac with Apple Silicon:

       ```
          import torch
          print(torch.backends.mps.is_available())
       ```
   * If it prints True, PyTorch can use the Metal Performance Shaders (MPS) backend for acceleration.

6. **Using Google Colab (no GPU at home)**
   * If you don’t have a GPU locally:
     - Open https://colab.research.google.com
     - Go to **Runtime → Change runtime type → Hardware accelerator → GPU**
     - Then run your PyTorch code with GPU support for free (time-limited).

       <img width="383" height="319" alt="image" src="https://github.com/user-attachments/assets/19330dcc-8b7b-451e-930a-ceff0e053fad" />

       
 7. **Common Techniques for Installing and Setting Up PyTorch**
    * Use **virtual environments** (e.g., venv or conda) to isolate dependencies.
    * **Match CUDA version** with your NVIDIA driver.
    * Always **test GPU availability** with torch.cuda.is_available().
    * **Pin library versions** for reproducibility (torch==2.4.0).
    * Use **Colab or cloud GPUs** if your machine lacks one.
    * **Update pip** before installation to avoid dependency errors:

       ```
          pip install --upgrade pip
       ```

     
     
