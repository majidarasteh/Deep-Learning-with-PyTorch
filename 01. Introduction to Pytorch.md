# Introduction to Pytorch

PyTorch is an open-source, **Python-based deep learning library** widely used for both research and production. According to Papers With Code, PyTorch has been the most widely used deep learning framework in research since 2019. Furthermore, the Kaggle Data Science and Machine Learning Survey 2022 reported that around $40%$ of respondents use PyTorch and this rate continues to grow each year. Reasons why PyTorch is popular:
1. **User-friendly interface**: Easy to learn and use
2. **Efficiency**: Optimized performance
3. **Flexibility**: Allows low-level customization while maintaining accessibility
4. **Balanced design**: Perfect blend of usability and advanced features

### The Three Core Components
PyTorch can be understood in terms of its three main components.
1. **Tensor Library**
   * **Foundation**: Extends NumPy's array-oriented programming.
   * **Key advantage**: Seamless GPU acceleration while maintaining CPU compatibility.
   * **Purpose**: Fundamental building block for all computations.
     
2. **Automatic Differentiation (autograd)**
   * **Function**: Automatically computes gradients for tensor operations.
   * **Benefit**: Simplifies backpropagation and model optimization.
   * **Impact**: Eliminates manual gradient calculation, making neural network training much easier.

3. **Deep Learning Library**
   * **Features**: Modular building blocks, pretrained models, loss functions, optimizers.
   * **Design philosophy**: Flexible and efficient components.
   * **Audience**: Caters to both researchers (flexibility) and developers (ease of use)

## Artificial Intelligence (AI)
PyTorch is a deep learning library used to build and train AI models such as LLM and CNN. For instance, Large Language Models (LLMs) are often referred to as AI models. However, LLMs are also a type of deep neural network, and PyTorch is a deep learning library used to build and train such models. This overlap in terminology can be confusing, so let’s briefly summarize the relationship between these terms before proceeding (the AI hierarchy):

1. **Artificial Intelligence (AI)**
   * Systems that perform tasks requiring human intelligence.
   * **Examples**: Natural language understanding, pattern recognition, decision making.
     
2. **Machine Learning (ML) - Subset of AI**
   * Algorithms that learn from data without explicit programming.
   * Improve performance over time with more data and feedback.
   * Real-world applications:
     - Recommendation systems (Netflix, Amazon)
     - Email spam filtering
     - Voice recognition (Siri, Alexa)
     - Self-driving cars
       
3. **Deep Learning (DL) - Subset of Machine Learning**
   * Deep neural networks with multiple hidden layers
   * Loosely modeled after the human brain's neural connections
   * Excellent at handling unstructured data (images, audio, text)
   * Multiple layers enabling complex, nonlinear relationships
   * Large Language Models (LLM) are a type of Deep Neural Network, which makes them part of this hierarchy.

    <img width="313" height="314" alt="image" src="https://github.com/user-attachments/assets/a05dfa4a-754e-455a-a1fc-738cf449bfaf" />


 ### Supervised and Unsupervised Learning
 In **machine learning**, algorithms are often categorized based on the type of data available during training and the nature of the learning objective. Two of the most common paradigms are **supervised learning** and **unsupervised learning**. Each serves different purposes and is suited for different types of tasks.

1. **Supervised Learning**
   * Supervised learning involves training a model on a dataset that contains both **inputs (features)** and their corresponding **outputs (labels)**.
   * The model learns to map inputs to outputs by minimizing the difference between its predictions and the true labels.
   * **Common Tasks:**
     - **Classification**: Predict discrete categories (spam/not spam, cat/dog)
     - **Regression**: Predict continuous values (house prices, temperature)
       
   * **Example**
     - In an email spam classifier, each training sample (email) has a label—either “spam” or “not spam.”
     - LLM pretraining via next-word prediction (sequence → next word)
     - In image classification, the input might be an image, and the label could be a class such as “cat”, “dog”, or “car.”
       
   * **Common algorithms and models include:**
     - Linear regression and logistic regression
     - Decision trees and random forests
     - Support vector machines (SVMs)
     - Neural networks and deep learning architectures (e.g., CNNs, LSTMs, Transformers)

2. **Unsupervised Learning**
   * Unsupervised learning deals with data that does not contain labels.
   * The algorithm explores the structure of the data on its own to find patterns, relationships, or groupings among examples.
   * The goal is to uncover hidden structure rather than predict explicit outcomes.
   * Unlike supervised learning, where feedback is provided in the form of correct labels, unsupervised learning relies on data similarity and statistical relationships to discover structure.
     
   * **Common Tasks:**
     - **Clustering**: Group similar data points (customer segmentation)
     - **Anomaly Detection**: Find unusual data points
     - **Association: Discover** relationships between variables
       
   * **Examples:**
     - Grouping news articles by topic without knowing categories
     - Finding similar customers based on purchasing behavior
     - Reducing image dimensions while preserving important features
       
   * **Common algorithms**
     - K-means, hierarchical clustering, DBSCAN, ...

While supervised and unsupervised learning represent two ends of the machine learning spectrum, many real-world problems **fall somewhere in between**. In practice, **labeled data is often scarce** or expensive to obtain, while unlabeled data is abundant. This challenge has led to the development of **semi-supervised** and **self-supervised learning** techniques, both of which aim to make better use of available unlabeled data.

3. **Semi-supervised Learning**
   * Semi-supervised learning combines aspects of both supervised and unsupervised learning.
   * It leverages a small amount of labeled data together with a large amount of unlabeled data to improve model performance.
   * The key idea is that the model first learns the general structure or distribution of the data from the unlabeled portion and then refines its understanding using the limited labeled examples.
     
    * **Real-world Example:**
      - Medical imaging: 100 labeled tumor scans + 10,000 unlabeled scans
      - Text classification: 1,000 labeled customer reviews + 1 million unlabeled reviews
    * Semi-supervised learning has proven particularly effective when obtaining labeled data is costly or time-consuming, such as in healthcare, fraud detection, and natural language processing (NLP).
    * **Common techniques**
      - Self-training: Model labels its own predictions on unlabeled data, then retrains
      - Co-training: Multiple models teach each other using different data views

4. **Self-supervised Learning**
   * **Core Idea**: Generate labels automatically from the data itself.
   * **No human labels**: The data provides its own supervision signal
   * **Examples:**
     - Predict missing words in sentences
     - Next sentence prediction
     - Recover missing parts of images



| Learning Type       | Data Requirement                       | Uses Labels          | Example Tasks                        | Typical Use Case                        |
| ------------------- | -------------------------------------- | -------------------- | ------------------------------------ | --------------------------------------- |
| **Supervised**      | Labeled data only                      | ✅ Yes                | Classification, regression           | Spam detection, object recognition      |
| **Unsupervised**    | Unlabeled data only                    | ❌ No                 | Clustering, dimensionality reduction | Customer segmentation, topic modeling   |
| **Semi-supervised** | Mix of labeled + unlabeled             | ⚙️ Partially         | Classification with limited labels   | Medical imaging, fraud detection        |
| **Self-supervised** | Unlabeled data (auto-generated labels) | ⚙️ Derived from data | Representation learning, pretraining | LLMs, vision transformers, autoencoders |

## Installing PyTorch
**PyTorch** is a large, flexible deep learning library with both **CPU** and **GPU** support. Because of that, installation depends on your system’s hardware and Python environment.

1. **Python Version Compatibility**
   * Use a stable version that’s one or two releases behind the newest one.
   * For example, if the latest is Python 3.13, install Python 3.11 or 3.12 for best compatibility.
   * Reason: Scientific libraries take time to support newest Python versions
     
  2. **Basic CPU-only installation**
     * If you don’t have a GPU or don’t need GPU acceleration:

       ```
          pip install torch
       ```
     * Installing with **CUDA for NVIDIA GPUs
        - Go to https://pytorch.org
        - Use the installation selector to get the correct command for your OS and CUDA version.
        - Example (for PyTorch 2.4.0):

       ```
          pip install torch==2.4.0 torchvision torchaudio
       ```
  3. **Verify installation**
       * Run this in Python:
    
       ```
          import torch
          print(torch.__version__)
       ```
       * Expected output:
    
       ```
          '2.4.0'
       ```
       
4. **Check GPU availability**
   * To verify GPU support:

       ```
          import torch
          print(torch.cuda.is_available())
       ```
   * If it prints True, GPU acceleration is active.
   * If it prints False, check your GPU drivers, CUDA, or reinstall with the right CUDA version.
  
5. **Apple Silicon (M1/M2/M3) acceleration**
   * If using a Mac with Apple Silicon:

       ```
          import torch
          print(torch.backends.mps.is_available())
       ```
   * If it prints True, PyTorch can use the Metal Performance Shaders (MPS) backend for acceleration.

6. **Using Google Colab (no GPU at home)**
   * If you don’t have a GPU locally:
     - Open https://colab.research.google.com
     - Go to **Runtime → Change runtime type → Hardware accelerator → GPU**
     - Then run your PyTorch code with GPU support for free (time-limited).

       <img width="383" height="319" alt="image" src="https://github.com/user-attachments/assets/19330dcc-8b7b-451e-930a-ceff0e053fad" />

       
 7. **Common Techniques for Installing and Setting Up PyTorch**
    * Use **virtual environments** (e.g., venv or conda) to isolate dependencies.
    * **Match CUDA version** with your NVIDIA driver.
    * Always **test GPU availability** with torch.cuda.is_available().
    * **Pin library versions** for reproducibility (torch==2.4.0).
    * Use **Colab or cloud GPUs** if your machine lacks one.
    * **Update pip** before installation to avoid dependency errors:

       ```
          pip install --upgrade pip
       ```

## Tensor Creation and Data Types
**Tensors are the fundamental data structure** used in PyTorch and all modern deep learning frameworks.
They generalize mathematical structures like **scalars, vectors, and matrixes** to higher dimensions.

A tensor is a** multidimensional array** — a mathematical object that can be described by its **rank (order)**, which indicates **how many dimensions (or axes)** it has.

| Tensor Type | Rank (Dimensions) | Example                          | Shape       |
| ----------- | ----------------- | -------------------------------- | ----------- |
| Scalar      | 0D                | `5`                              | `()`        |
| Vector      | 1D                | `[1, 2, 3]`                      | `(3,)`      |
| Matrix      | 2D                | `[[1, 2], [3, 4]]`               | `(2, 2)`    |
| 3D Tensor   | 3D                | `[[[1,2],[3,4]], [[5,6],[7,8]]]` | `(2, 2, 2)` |

**Interpretation:**
1. **Scalars (0D)** store single values.
2. **Vectors (1D)** store ordered lists of numbers.
3. **Matrixes (2D)** represent grids of numbers.
4. **Higher-rank tensors (e.g 3D)** represent multi-dimensional data such as images, sequences, or batches.

### Computational Perspective
1. **Data Containers**
   * From a computing viewpoint, **tensors are containers for numerical data**.
   * Each dimension corresponds to a **feature or property** of that data.
   * Essential for **representing complex data** like images, text, etc.
   * **Examples:**
     - **A 1D tensor** → a list of temperatures over time.
     - **A 2D tensor** → a grayscale image (height × width).
     - **A 3D tensor** → an RGB image (height × width × 3 channels).
     - **A 4D tensor** → a batch of RGB images.

2. **PyTorch vs NumPy Comparison**
   * **Similarities:**
     - Similar API and syntax
     - Support similar operations (indexing, slicing, mathematical ops)
     - Can often convert between them easily
   * **PyTorch Advantages:**
     - **GPU acceleration** - Allow execution on **CPUs** and **GPUs**,
     - **Automatic differentiation** - Enable automatic differentiation for deep learning.
     - **Better for deep learning** - Optimized for neural networks
     - Support efficient mathematical computations

## Common Data Types in PyTorch
```python
import torch

# Floating point types
float16 = torch.tensor([1, 2, 3], dtype=torch.float16)    # Half precision
float32 = torch.tensor([1, 2, 3], dtype=torch.float32)    # Single precision (DEFAULT)
float64 = torch.tensor([1, 2, 3], dtype=torch.float64)    # Double precision
bfloat16 = torch.tensor([1, 2, 3], dtype=torch.bfloat16)  # Brain float

# Integer types
int8 = torch.tensor([1, 2, 3], dtype=torch.int8)          # 8-bit integer
int16 = torch.tensor([1, 2, 3], dtype=torch.int16)        # 16-bit integer
int32 = torch.tensor([1, 2, 3], dtype=torch.int32)        # 32-bit integer
int64 = torch.tensor([1, 2, 3], dtype=torch.int64)        # 64-bit integer (DEFAULT)

# Other types
bool_type = torch.tensor([True, False], dtype=torch.bool) # Boolean
uint8 = torch.tensor([1, 2, 3], dtype=torch.uint8)        # Unsigned 8-bit integer

print("Float32 size:", float32.element_size(), "bytes")
print("Float64 size:", float64.element_size(), "bytes")
print("Int64 size:", int64.element_size(), "bytes")
```

**Expected output**

```
Float32 size: 4 bytes
Float64 size: 8 bytes
Int64 size: 8 bytes
```

### Practical Tensor Examples: Scalars, Vectors, Matrixes, and Tensors
PyTorch tensors are **created using** the `torch.tensor()` function. Each rank of a tensor corresponds to how nested your data structure is.

```python
import torch

tensor0d = torch.tensor(1)                   # 0D tensor (scalar)
tensor1d = torch.tensor([1, 2, 3])           # 1D tensor (vector)
tensor2d = torch.tensor([[1, 2],
                         [3, 4]])            # 2D tensor (matrix)
tensor3d = torch.tensor([[[1, 2], [3, 4]],
                         [[5, 6], [7, 8]]])  # 3D tensor

print("0D tensor: \n", tensor0d)
print("\n1D tensor: \n", tensor1d)
print("\n2D tensor: \n", tensor2d)
print("\n3D tensor: \n", tensor3d)

```

**Expected output:**
```
0D tensor: 
 tensor(1)

1D tensor: 
 tensor([1, 2, 3])

2D tensor: 
 tensor([[1, 2],
        [3, 4]])

3D tensor: 
 tensor([[[1, 2],
         [3, 4]],

        [[5, 6],
         [7, 8]]])
```

## Creating Tensors with Float Data Type in PyTorch
There are several ways to create tensors with float data types in PyTorch. Here's a comprehensive guide:

**For most deep learning applications, use float32**

**Explicitly Specify dtype during Creation**

```python
import torch

# Method 1: Using dtype parameter
float_tensor1 = torch.tensor([1, 2, 3, 4], dtype=torch.float32)
float_tensor2 = torch.tensor([1.5, 2.7, 3.9], dtype=torch.float64)

print("Float32 tensor:", float_tensor1, float_tensor1.dtype)
print("Float64 tensor:", float_tensor2, float_tensor2.dtype)
```

**Expected output**

```
Float32 tensor: tensor([1., 2., 3., 4.]) torch.float32
Float64 tensor: tensor([1.5000, 2.7000, 3.9000], dtype=torch.float64) torch.float64
```

**Using Python Float Literals**

```python
# PyTorch automatically infers float32 from Python floats
auto_float = torch.tensor([1.0, 2.0, 3.0])  # Automatically float32
mixed = torch.tensor([1, 2.5, 3])           # Mixed -> promotes to float32

print("Auto float:", auto_float.dtype)      # torch.float32
print("Mixed types:", mixed.dtype)          # torch.float32
```

**Expected output**

```
Auto float: torch.float32
Mixed types: torch.float32
```

## Properties of a Tensor
A tensor in PyTorch has several important properties that describe its structure and contents:

| **Property**  | **Method / Attribute** | **Description**                                        | **Example Output**   |
| ------------- | ---------------------- | ------------------------------------------------------ | -------------------- |
| **Shape**     | `.shape`               | Size (length) of each dimension                        | `torch.Size([2, 3])` |
| **Rank**      | `.dim()`               | Number of dimensions (axes)                            | `2`                  |
| **Size**      | `.size()`              | Alias for `.shape`, gives tensor dimensions            | `torch.Size([2, 3])` |
| **Elements**  | `.numel()`             | Total number of elements in the tensor                 | `6`                  |
| **Data Type** | `.dtype`               | Type of data stored in tensor elements                 | `torch.float32`      |
| **Device**    | `.device`              | Indicates where the tensor is stored (`cpu` or `cuda`) | `device(type='cpu')` |


### For example consider the following code**

```python
import torch

# Example 1: 1D Tensor (Vector)
vector = torch.tensor([1, 2, 3, 4, 5])
print("=== 1D Tensor ===")
print("Tensor:", vector)
print("Shape:", vector.shape)                 # torch.Size([5])
print("Rank:", vector.dim())                  # 1
print("Size:", vector.size())                 # torch.Size([5])
print("Number of elements:", vector.numel())  # 5
print("Data type: ", vector.dtype)            # torch.int64
print("Device: ", vector.device)              # cpu


# Example 2: 2D Tensor (Matrix)
matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])
print("\n=== 2D Tensor ===")
print("Tensor:", matrix)
print("Shape:", matrix.shape)                # torch.Size([2, 3])
print("Rank:", matrix.dim())                 # 2
print("Size:", matrix.size())                # torch.Size([2, 3])
print("Number of elements:", matrix.numel()) # 6
print("Data type: ", matrix.dtype)           # torch.int64
print("Device: ", matrix.device)              # cpu

# Example 3: 3D Tensor
tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
print("\n=== 3D Tensor ===")
print("Tensor:", tensor_3d)
print("Shape:", tensor_3d.shape)                # torch.Size([2, 2, 2])
print("Rank:", tensor_3d.dim())                 # 3
print("Size:", tensor_3d.size())                # torch.Size([2, 2, 2])
print("Number of elements:", tensor_3d.numel()) # 8
print("Data type: ", tensor_3d.dtype)              # torch.int64
print("Device: ", tensor_3d.device)              # cpu
```

**Expected output**

```
=== 1D Tensor ===
Tensor: tensor([1, 2, 3, 4, 5])
Shape: torch.Size([5])
Rank: 1
Size: torch.Size([5])
Number of elements: 5
Data type:  torch.int64
Device:  cpu

=== 2D Tensor ===
Tensor: tensor([[1, 2, 3],
        [4, 5, 6]])
Shape: torch.Size([2, 3])
Rank: 2
Size: torch.Size([2, 3])
Number of elements: 6
Data type:  torch.int64
Device:  cpu

=== 3D Tensor ===
Tensor: tensor([[[1, 2],
         [3, 4]],

        [[5, 6],
         [7, 8]]])
Shape: torch.Size([2, 2, 2])
Rank: 3
Size: torch.Size([2, 2, 2])
Number of elements: 8
Data type:  torch.int64
Device:  cpu
```

## Tensor Indexing and Element Access 
Tensor indexing in PyTorch allows you to access and manipulate specific elements or subsets of your data with intuitive, NumPy-like syntax. Think of tensors as multi-dimensional containers where each dimension can be accessed using square brackets `[]`.
1. For 1D tensors (vectors), use single indices like `tensor[0]` to get the first element.
2. For 2D tensors (matrices), use comma-separated indices like `tensor[1, 2]` to access the element at row 1, column 2.
3. Slicing with colons: `tensor[:3]` gets the first three elements.
4. `tensor[:, 1]` gets the entire second column, and `tensor[::2]` gets every other element.
5. For more advanced selection, boolean masking lets you filter elements based on conditions (`tensor[tensor > 5]`), while integer array indexing allows picking specific indices.

**Examples:**

1. **1D tensor**

```python
import torch

# 1D tensor
vector = torch.tensor([10, 20, 30, 40, 50])
print("Original vector:", vector)

# Access single element
print("vector[2]:", vector[2])        # tensor(30)
print("vector[-1]:", vector[-1])      # tensor(50) - last element

# Access range of elements (slicing)
print("vector[1:4]:", vector[1:4])    # tensor([20, 30, 40])
print("vector[:3]:", vector[:3])      # tensor([10, 20, 30]) - first 3
print("vector[2:]:", vector[2:])      # tensor([30, 40, 50]) - from index 2
```

**Expected output:**
```
Original vector: tensor([10, 20, 30, 40, 50])
vector[2]: tensor(30)
vector[-1]: tensor(50)
vector[1:4]: tensor([20, 30, 40])
vector[:3]: tensor([10, 20, 30])
vector[2:]: tensor([30, 40, 50])
```

2. **2D Tensor**

```python
# 2D tensor (matrix)
matrix = torch.tensor([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9]])
print("Original matrix:")
print(matrix)

# Access single element
print("matrix[0, 1]:", matrix[0, 1])      # tensor(2) - row 0, column 1
print("matrix[2, 0]:", matrix[2, 0])      # tensor(7) - row 2, column 0

# Access entire row
print("matrix[1]:", matrix[1])            # tensor([4, 5, 6]) - row 1
print("matrix[1, :]:", matrix[1, :])      # Same as above

# Access entire column
print("matrix[:, 2]:", matrix[:, 2])      # tensor([3, 6, 9]) - column 2
```

**Expected output**

```
Original matrix:
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
matrix[0, 1]: tensor(2)
matrix[2, 0]: tensor(7)
matrix[1]: tensor([4, 5, 6])
matrix[1, :]: tensor([4, 5, 6])
matrix[:, 2]: tensor([3, 6, 9])
```

3. **3D tensor**

```python
# 3D tensor (batch, rows, columns)
tensor_3d = torch.tensor([[[1, 2], [3, 4]],
                          [[5, 6], [7, 8]],
                          [[9, 10], [11, 12]]])
print("3D tensor shape:", tensor_3d.shape)          # torch.Size([3, 2, 2])

# Access different dimensions
print("First batch:", tensor_3d[0])                 # tensor([[1, 2], [3, 4]])
print("Second batch, first row:", tensor_3d[1, 0])  # tensor([5, 6])
print("All batches, first row:", tensor_3d[:, 0])   # tensor([[1, 2], [5, 6], [9, 10]])
print("Specific element:", tensor_3d[2, 1, 0])      # tensor(11)
```

**Expected output**

```
3D tensor shape: torch.Size([3, 2, 2])
First batch: tensor([[1, 2],
        [3, 4]])
Second batch, first row: tensor([5, 6])
All batches, first row: tensor([[ 1,  2],
        [ 5,  6],
        [ 9, 10]])
Specific element: tensor(11)
```
4. **Boolean Masking**

```python
# Create a tensor
data = torch.tensor([5, 12, 8, 3, 15, 1])

# Boolean masking
mask = data > 7
print("Mask:", mask)                            # tensor([False, True, True, False, True, False])
print("Values > 7:", data[mask])                # tensor([12, 8, 15])

# Multiple conditions
mask2 = (data > 5) & (data < 10)
print("Values between 5 and 10:", data[mask2])  # tensor([8])

# Direct condition
print("Even numbers:", data[data % 2 == 0])     # tensor([12, 8])
```

**Expected output**

```
Mask: tensor([False,  True,  True, False,  True, False])
Values > 7: tensor([12,  8, 15])
Values between 5 and 10: tensor([8])
Even numbers: tensor([12,  8])
```
5. **Integer Array Indexing**

```python
tensor = torch.tensor([[10, 20, 30],
                       [40, 50, 60],
                       [70, 80, 90]])

# Select specific indices
print("Rows [0, 2], all columns:", tensor[[0, 2]])      # Rows 0 and 2, all columns
print("Columns [1, 2] of all rows:", tensor[:, [1, 2]]) # All rows, columns 1 and 2

# Advanced indexing - select specific elements
rows = torch.tensor([0, 1, 2])
cols = torch.tensor([1, 0, 2])

# From row o, select column 1 and From row 1, select column 0 and From row 2, select column 2
print("Specific elements:", tensor[rows, cols])         # [20, 40, 90]
```

**Expected output**

```
Rows [0, 2], all columns: tensor([[10, 20, 30],
        [70, 80, 90]])
Columns [1, 2] of all rows: tensor([[20, 30],
        [50, 60],
        [80, 90]])
Specific elements: tensor([20, 40, 90])
```

### Modifying Tensor Values in PyTorch
There are several ways to modify tensor values in PyTorch. Here's a comprehensive guide:
1. **Direct Assignment by Index - Single Element Modification**

```python
import torch

# Create a tensor
tensor = torch.tensor([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9]])
print("Original tensor:")
print(tensor)

# Modify single element
tensor[0, 1] = 99
print("\nAfter modifying [0,1]:")
print(tensor)
```
**Expected output**

```
Original tensor:
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])

After modifying [0,1]:
tensor([[ 1, 99,  3],
        [ 4,  5,  6],
        [ 7,  8,  9]])
```

2. **Direct Assignment by Index - Row/Column Modification**

```python
# Modify entire row
tensor[1] = torch.tensor([10, 20, 30])
print("\nAfter modifying row 1:")
print(tensor)

# Modify entire column
tensor[:, 2] = torch.tensor([100, 200, 300])
print("\nAfter modifying column 2:")
print(tensor)

# Modify entire column with a unique number
tensor[:, 1] = torch.tensor(-1)
print("\nAfter modifying column 1:")
print(tensor)
```

**Expected output**

```
After modifying row 1:
tensor([[ 1, 99, -1],
        [10, 20, 30],
        [ 7,  8, -1]])

After modifying column 2:
tensor([[  1,  99, 100],
        [ 10,  20, 200],
        [  7,   8, 300]])

After modifying column 1:
tensor([[  1,  -1, 100],
        [ 10,  -1, 200],
        [  7,  -1, 300]])
```
3. **Slicing and Range Modification**

```python
# Create new tensor
tensor = torch.tensor([[1, 2, 3, 4],
                       [5, 6, 7, 8],
                       [9, 10, 11, 12]])

# Modify a slice
tensor[0:2, 1:3] = torch.tensor([[20, 30], [60, 70]])
print("After slice modification:")
print(tensor)
```

**Output**

```
After slice modification:
tensor([[ 1, 20, 30,  4],
        [ 5, 60, 70,  8],
        [ 9, 10, 11, 12]])
```

4. **Boolean Masking Modification** 

```python
tensor = torch.tensor([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9]])

# Modify elements based on condition
print("\n============ Modify elements based on condition ==========\n")
mask = tensor > 5
tensor[mask] = -100
print("After setting values > 5 to 0:")
print(tensor)

print("\n================= Multiple conditions ==============\n")
# Multiple conditions
tensor[(tensor > 1) & (tensor < 4)] = 0
print("After multiple conditions:")
print(tensor)
```

**Expected output**

```
============ Modify elements based on condition ==========

After setting values > 5 to 0:
tensor([[   1,    2,    3],
        [   4,    5, -100],
        [-100, -100, -100]])

================= Multiple conditions ==============

After multiple conditions:
tensor([[   1,    0,    0],
        [   4,    5, -100],
        [-100, -100, -100]])
```

5. **In-place Operations**

```python
tensor = torch.tensor([[1.0, 2.0], 
                       [3.0, 4.0]])

# In-place arithmetic operations
tensor.add_(5)           # Add 5 to all elements (in-place)
print("After add_(5):")
print(tensor)

tensor.mul_(2)           # Multiply all elements by 2 (in-place)
print("\n After mul_(2):")
print(tensor)

# Other in-place operations
tensor.sub_(1)                      # Subtract 1
print("\n After sub_(1): \n", tensor)
tensor.div_(2)                      # Divide by 2
print("\n After div_(2): \n", tensor)
tensor.pow_(2)                      # Square all elements
print("\n After pow_(2): \n", tensor)
```

**Expected output**

```
After add_(5):
tensor([[6., 7.],
        [8., 9.]])

 After mul_(2):
tensor([[12., 14.],
        [16., 18.]])

 After sub_(1): 
 tensor([[11., 13.],
        [15., 17.]])

 After div_(2): 
 tensor([[5.5000, 6.5000],
        [7.5000, 8.5000]])

 After pow_(2): 
 tensor([[30.2500, 42.2500],
        [56.2500, 72.2500]])
```

6. **Using torch.where() for Conditional Modification**

```python
tensor = torch.tensor([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9]])

# Replace values based on condition
modified = torch.where(tensor > 5, 
                      torch.tensor(-10),  # Values where condition is True
                      tensor)            # Values where condition is False
print("Using torch.where():")
print(modified)
```

**Expected output**

```
Using torch.where():
tensor([[  1,   2,   3],
        [  4,   5, -10],
        [-10, -10, -10]])
```

7. **Mathematical Function Modifications** 

```python
tensor = torch.tensor([[1.0, 4.0],
                       [9.0, 16.0]])

# Apply mathematical functions (create new tensors)
sqrt_tensor = torch.sqrt(tensor)          # Square root
log_tensor = torch.log(tensor)            # Natural log
exp_tensor = torch.exp(tensor)            # Exponential

print("\n Square root:\n", sqrt_tensor)
print("\n Log:\n", log_tensor)
print("\n Exponential:\n", exp_tensor)
```

**Expected output**

```
Square root:
 tensor([[1., 2.],
        [3., 4.]])

 Log:
 tensor([[0.0000, 1.3863],
        [2.1972, 2.7726]])

 Exponential:
 tensor([[2.7183e+00, 5.4598e+01],
        [8.1031e+03, 8.8861e+06]])
```

8. **Filling Methods**

 ```python
tensor = torch.tensor([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9],
                       [10, 11, 12]])

# Fill with specific value
tensor.fill_(-5)
print("After fill_(0):")
print(tensor)

# Fill diagonal
tensor.fill_diagonal_(1)
print("\n After fill_diagonal_(1):")
print(tensor)

# Zero out specific elements
tensor.zero_()  # Fill entire tensor with zeros
print("\n After zero_():")
print(tensor)
```

**Expected output**

```
After fill_(0):
tensor([[-5, -5, -5],
        [-5, -5, -5],
        [-5, -5, -5],
        [-5, -5, -5]])

 After fill_diagonal_(1):
tensor([[ 1, -5, -5],
        [-5,  1, -5],
        [-5, -5,  1],
        [-5, -5, -5]])

 After zero_():
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
```

9. **zeros() and ones() tensors**

```python
tensor = torch.zeros(3,4)
print("zeros(3,4)\n", tensor)

tensor = torch.ones(3,4)
print("\nones(3,4)\n", tensor)
```

**Expected output**

```
zeros(3,4)
 tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])

ones(3,4)
 tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
```
10. **Memory Considerations (In place and outplace modification)**

```python
# In-place vs out-of-place operations
tensor = torch.tensor([1, 2, 3])

# Out-of-place: creates new tensor (more memory)
new_tensor = tensor + 1

# In-place: modifies existing tensor (less memory)
tensor.add_(1)

print("Same memory?", tensor.data_ptr() == new_tensor.data_ptr())  # False
```

**Expected output**

```
Same memory? False
```

## Converting Between NumPy and PyTorch
PyTorch makes it very easy to convert between NumPy arrays and PyTorch tensors. 
1. **NumPy → PyTorch Tensor**

**Shared Memory (from_numpy and as_tensor)**
     
```python
# NumPy to PyTorch with shared memory
numpy_arr = np.array([1, 2, 3, 4, 5])
torch_tensor = torch.from_numpy(numpy_arr)  # Shares memory

print("Original NumPy:", numpy_arr)
print("PyTorch tensor:", torch_tensor)

# Modify NumPy array
numpy_arr[0] = 99
print("\nAfter modifying NumPy:")
print("NumPy array:", numpy_arr)
print("PyTorch tensor:", torch_tensor)  # Also changed!

# Modify PyTorch tensor
torch_tensor[1] = 88
print("\nAfter modifying PyTorch:")
print("NumPy array:", numpy_arr)        # Also changed!
print("PyTorch tensor:", torch_tensor)
```

**Expected output**

```
Original NumPy: [1 2 3 4 5]
PyTorch tensor: tensor([1, 2, 3, 4, 5])

After modifying NumPy:
NumPy array: [99  2  3  4  5]
PyTorch tensor: tensor([99,  2,  3,  4,  5])

After modifying PyTorch:
NumPy array: [99 88  3  4  5]
PyTorch tensor: tensor([99, 88,  3,  4,  5])
```
**Separate Memory (torch.tensor)**

```python
# NumPy to PyTorch with separate memory
numpy_arr = np.array([1, 2, 3, 4, 5])
torch_tensor = torch.tensor(numpy_arr)  # Creates copy

print("Original NumPy:", numpy_arr)
print("PyTorch tensor:", torch_tensor)

# Modify NumPy array
numpy_arr[0] = 99
print("\nAfter modifying NumPy:")
print("NumPy array:", numpy_arr)
print("PyTorch tensor:", torch_tensor)  # Unchanged!

# Modify PyTorch tensor
torch_tensor[1] = 88
print("\nAfter modifying PyTorch:")
print("NumPy array:", numpy_arr)        # Unchanged!
print("PyTorch tensor:", torch_tensor)
```

**Expected output**

```
Original NumPy: [1 2 3 4 5]
PyTorch tensor: tensor([1, 2, 3, 4, 5])

After modifying NumPy:
NumPy array: [99  2  3  4  5]
PyTorch tensor: tensor([1, 2, 3, 4, 5])

After modifying PyTorch:
NumPy array: [99  2  3  4  5]
PyTorch tensor: tensor([ 1, 88,  3,  4,  5])
```

2. **PyTorch → NumPy**

```python
# Create a PyTorch tensor
torch_tensor = torch.tensor([[1.0, 2.0, 3.0],
                             [4.0, 5.0, 6.0]])
print("PyTorch tensor:")
print(torch_tensor)
print("Type:", type(torch_tensor))

# Convert to NumPy array
numpy_array = torch_tensor.numpy()
print("\nNumPy array:")
print(numpy_array)
print("Type:", type(numpy_array))
print("Shape:", numpy_array.shape)
print("Dtype:", numpy_array.dtype)
```

**Expected output**

```
PyTorch tensor:
tensor([[1., 2., 3.],
        [4., 5., 6.]])
Type: <class 'torch.Tensor'>

NumPy array:
[[1. 2. 3.]
 [4. 5. 6.]]
Type: <class 'numpy.ndarray'>
Shape: (2, 3)
Dtype: float32
```

## Changing Tensor Shape in PyTorch
PyTorch provides several methods to reshape tensors. 

1. **PyTorch provides several methods to reshape tensors - Using .reshape()**

```python
import torch

# Original tensor: 3x4
original = torch.tensor([[1, 2, 3, 4],
                         [5, 6, 7, 8],
                         [9, 10, 11, 12]])
print("Original shape (3x4):", original.shape)
print(original)

# Reshape to 2x3x2
reshaped = original.reshape(2, 3, 2)
print("\nReshaped to (2x3x2):", reshaped.shape)
print(reshaped)
```

**Expected output**

```
Original shape (3x4): torch.Size([3, 4])
tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])

Reshaped to (2x3x2): torch.Size([2, 3, 2])
tensor([[[ 1,  2],
         [ 3,  4],
         [ 5,  6]],

        [[ 7,  8],
         [ 9, 10],
         [11, 12]]])
```

2. **PyTorch provides several methods to reshape tensors - Using .view()**

```python
# Using view() - requires contiguous memory
reshaped_view = original.view(2, 3, 2)
print("Using view() - same result:")
print(reshaped_view.shape)
print(reshaped_view)
```

**Expected output**

```
Using view() - same result:
torch.Size([2, 3, 2])
tensor([[[ 1,  2],
         [ 3,  4],
         [ 5,  6]],

        [[ 7,  8],
         [ 9, 10],
         [11, 12]]])
```

3. **Flattening Tensors**

```python
# Flatten to 1D
flattened = original.reshape(-1)      # or original.view(-1)
print("Flattened:", flattened.shape)  # torch.Size([12])
print(flattened)  # tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])

# Flatten specific dimensions
flattened_2d = original.reshape(3, -1)      # Keep first dim, flatten rest
print("Flattened 2D:", flattened_2d.shape)  # torch.Size([3, 4])
print(flattened_2d)
```

**Expected output**

```
Flattened: torch.Size([12])
tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])
Flattened 2D: torch.Size([3, 4])
tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])
```

4. **Adding/Removing Dimensions - unsqueeze(0)/squeeze(0)**

```python
print("Original tensor's dim:", original.shape)  # torch.Size([3, 4])
print(original)
# Add new dimensions
with_batch = original.reshape(1, 3, 4)      # Add batch dimension
print("\nWith batch dim:", with_batch.shape)  # torch.Size([1, 3, 4])

# Using unsqueeze()
with_batch2 = original.unsqueeze(0)           # Add dimension at position 0
print("Using unsqueeze:", with_batch2.shape)  # torch.Size([1, 3, 4])
print(with_batch2)

# Remove dimensions of size 1
squeezed = with_batch2.squeeze(0)        # Remove dimension at position 0
print("\nAfter squeeze:", squeezed.shape)  # torch.Size([3, 4])
print("\nAfter squeeze:\n", squeezed)  
```

**Expected output**

```
Original tensor's dim: torch.Size([3, 4])
tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])

With batch dim: torch.Size([1, 3, 4])
Using unsqueeze: torch.Size([1, 3, 4])
tensor([[[ 1,  2,  3,  4],
         [ 5,  6,  7,  8],
         [ 9, 10, 11, 12]]])

After squeeze: torch.Size([3, 4])

After squeeze:
 tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])
```
5. **Automatic Dimension Calculation**

```python
# Let PyTorch calculate one dimension
original = torch.tensor([[1, 2, 3, 4],
                         [5, 6, 7, 8],
                         [9, 10, 11, 12]])

# Use -1 for automatic dimension calculation
auto1 = original.reshape(2, -1)    # 2x6
auto2 = original.reshape(-1, 6)    # 2x6
auto3 = original.reshape(2, 3, -1) # 2x3x2

print("2 x -1:", auto1.shape)      # torch.Size([2, 6])
print("-1 x 6:", auto2.shape)      # torch.Size([2, 6])
print("2 x 3 x -1:", auto3.shape)  # torch.Size([2, 3, 2])
```

**Expected output**

```
2 x -1: torch.Size([2, 6])
-1 x 6: torch.Size([2, 6])
2 x 3 x -1: torch.Size([2, 3, 2])
```
6. **Basic Stacking - torch.stack()**

```python
import torch

# Create individual tensors
tensor1 = torch.tensor([1, 2, 3])
tensor2 = torch.tensor([4, 5, 6])
tensor3 = torch.tensor([7, 8, 9])

print("Individual tensors:")
print("tensor1:", tensor1)
print("tensor2:", tensor2)
print("tensor3:", tensor3)

# Stack along a new dimension (default dim=0)
stacked = torch.stack([tensor1, tensor2, tensor3])
print("\nStacked (dim=0):")
print(stacked)
print("\nShape:", stacked.shape)  # torch.Size([3, 3])
```

**Expected output**

```
Individual tensors:
tensor1: tensor([1, 2, 3])
tensor2: tensor([4, 5, 6])
tensor3: tensor([7, 8, 9])

Stacked (dim=0):
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])

Shape: torch.Size([3, 3])
```

7. **Concatenation (Different from Stacking)**

```python
# Understanding the difference between stack and cat
tensor1 = torch.tensor([1, 2, 3])
tensor2 = torch.tensor([4, 5, 6])

# Concatenate (joins along existing dimension)
concatenated = torch.cat([tensor1, tensor2])
print("Concatenated:", concatenated)              # tensor([1, 2, 3, 4, 5, 6])
print("Concatenated shape:", concatenated.shape)  # torch.Size([6])

# Stack (creates new dimension)
stacked = torch.stack([tensor1, tensor2])
print("\nStacked:\n", stacked)
print("\nStacked shape:", stacked.shape)    # torch.Size([2, 3])
```

**Expected output**

```
Concatenated: tensor([1, 2, 3, 4, 5, 6])
Concatenated shape: torch.Size([6])

Stacked:
 tensor([[1, 2, 3],
        [4, 5, 6]])

Stacked shape: torch.Size([2, 3])
```

## Transpose in PyTorch
PyTorch provides several methods for transposing tensors. 
1. **Simple Transpose (2D Tensors) - .T**

```python
import torch

matrix = torch.tensor([[1, 2, 3],
                       [4, 5, 6]])

print("Original matrix: \n", matrix)
print("\nShape:", matrix.shape)      # torch.Size([2, 3])

# Transpose using .T
transposed = matrix.T
print("\nTransposed (.T): \n", transposed)
print("\nShape:", transposed.shape)  # torch.Size([3, 2])

# Different from reshape
reshaped_matrix = matrix.reshape(3,2)
print(f"\nReshape matrix (3*2) is different from transpose and reshape is:\n {reshaped_matrix} and its shape is: \n {reshaped_matrix.shape}")
```

**Expected output**

```
Original matrix: 
 tensor([[1, 2, 3],
        [4, 5, 6]])

Shape: torch.Size([2, 3])

Transposed (.T): 
 tensor([[1, 4],
        [2, 5],
        [3, 6]])

Shape: torch.Size([3, 2])

Reshape matrix (3*2) is different from transpose and reshape is:
 tensor([[1, 2],
        [3, 4],
        [5, 6]]) and its shape is: 
 torch.Size([3, 2])
```

2. **Also for 2D Transpose - .t()**

```python
# Alternative for 2D tensors
matrix = torch.tensor([[1, 2], [3, 4]])
transposed_t = matrix.t()
print("Transpose using .t():")
print(transposed_t)
```

**Expected output**

```
Transpose using .t():
tensor([[1, 3],
        [2, 4]])
```

3. **General Transpose - torch.transpose()**

```python
# 3D tensor
tensor_3d = torch.tensor([[[1, 2], [3, 4]],
                          [[5, 6], [7, 8]]])
print("Original 3D:")
print(tensor_3d)
print("Shape:", tensor_3d.shape)                    # torch.Size([2, 2, 2])

# Transpose specific dimensions
transposed_3d = torch.transpose(tensor_3d, 0, 1)    # Swap dim0 and dim1
print("\nTransposed dim0 and dim1:")
print(transposed_3d)
print("Shape:", transposed_3d.shape)                # torch.Size([2, 2, 2])

# Transpose different dimensions
transposed_3d_2 = torch.transpose(tensor_3d, 1, 2)  # Swap dim1 and dim2
print("\nTransposed dim1 and dim2:")
print(transposed_3d_2)
print("Shape:", transposed_3d_2.shape)              # torch.Size([2, 2, 2])
```

**Expected output**

```
Original 3D:
tensor([[[1, 2],
         [3, 4]],

        [[5, 6],
         [7, 8]]])
Shape: torch.Size([2, 2, 2])

Transposed dim0 and dim1:
tensor([[[1, 2],
         [5, 6]],

        [[3, 4],
         [7, 8]]])
Shape: torch.Size([2, 2, 2])

Transposed dim1 and dim2:
tensor([[[1, 3],
         [2, 4]],

        [[5, 7],
         [6, 8]]])
Shape: torch.Size([2, 2, 2])
```

## Main Mathematical Operations in PyTorch
Here are the essential mathematical operations in PyTorch:

1. **1. Basic Arithmetic Operations**

```python
import torch

a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

# Element-wise operations
add = a + b           # Addition: [5, 7, 9]
subtract = a - b      # Subtraction: [-3, -3, -3]
multiply = a * b      # Multiplication: [4, 10, 18]
divide = a / b        # Division: [0.25, 0.4, 0.5]
power = a ** b        # Power: [1, 32, 729]
modulo = a % 2        # Modulo: [1, 0, 1]

print("First tensor: ", a)
print("Second tensor: ", b)

print("\nAdd: ", add)
print("Subtract: ", subtract)
print("Multiply: ", multiply)
print("Divide: ", divide)
print("Power: ", power)
print("Modulo: ", modulo)


# Equivalent function forms
add_func = torch.add(a, b)
sub_func = torch.sub(a, b)
mul_func = torch.mul(a, b)
div_func = torch.div(a, b)
```

**Expected output**

```
First tensor:  tensor([1, 2, 3])
Second tensor:  tensor([4, 5, 6])

Add:  tensor([5, 7, 9])
Subtract:  tensor([-3, -3, -3])
Multiply:  tensor([ 4, 10, 18])
Divide:  tensor([0.2500, 0.4000, 0.5000])
Power:  tensor([  1,  32, 729])
Modulo:  tensor([1, 0, 1])
```

2. **In-place Operations**

```python
x = torch.tensor([1.0, 2.0, 3.0])

# In-place operations (modify original tensor)
x.add_(1)            # x = x + 1 → [2., 3., 4.]
x.sub_(0.5)          # x = x - 0.5 → [1.5, 2.5, 3.5]
x.mul_(2)            # x = x * 2 → [3., 5., 7.]
x.div_(2)            # x = x / 2 → [1.5, 2.5, 3.5]
x.pow_(2)            # x = x² → [2.25, 6.25, 12.25]
```

**Expected output**

```
tensor([ 2.2500,  6.2500, 12.2500])
```

3. **Matrix Operations -  Matrix multiplication - Using @**

```python
# Matrix operations Using @
A = torch.tensor([[1, 2], [3, 4], [5, 6]])             # 3*2
B = torch.tensor([[7, 8, 9, 10], [11, 12, 13, 14]])   # 2*4

# Matrix multiplication
matmul = A @ B                                        # Result: 3*4

print("Matrix A: \n", A)
print("\nMatrix B: \n", B)
print("\n Matrix multiplication A and B: \n", matmul)
```

**Expected output**

```
Matrix A: 
 tensor([[1, 2],
        [3, 4],
        [5, 6]])

Matrix A: 
 tensor([[ 7,  8,  9, 10],
        [11, 12, 13, 14]])

 Matrix multiplication A and B: 
 tensor([[ 29,  32,  35,  38],
        [ 65,  72,  79,  86],
        [101, 112, 123, 134]])
```

4. **Matrix Operations - Matrix multiplication - Using torch.matmul(A, B)**

```python
# Matrix multiplication - Using torch.matmul(A, B)

A = torch.tensor([[1, 2], [3, 4], [5, 6]])            # 3*2
B = torch.tensor([[7, 8, 9, 10], [11, 12, 13, 14]])   # 2*4

# Matrix multiplication
matmul = torch.matmul(A, B)                           # Result: 3*4 

print("Matrix A: \n", A)
print("\nMatrix B: \n", B)
print("\n Matrix multiplication A and B: \n", matmul)
```

**Expected output**

```
Matrix A: 
 tensor([[1, 2],
        [3, 4],
        [5, 6]])

Matrix A: 
 tensor([[ 7,  8,  9, 10],
        [11, 12, 13, 14]])

 Matrix multiplication A and B: 
 tensor([[ 29,  32,  35,  38],
        [ 65,  72,  79,  86],
        [101, 112, 123, 134]])
```

5. **Matrix Operations - Dot product**

```python
# Matrix Dot product
vector1 = torch.tensor([1, 2, 3])
vector2 = torch.tensor([4, 5, 6])

dot_product = torch.dot(vector1, vector2)  # 1*4 + 2*5 + 3*6 = 32

print("Vector A: \n", vector1)
print("\nVector B: \n", vector2)
print("\nDot product \n", dot_product)
```

**Expected output**

```
Vector A: 
 tensor([1, 2, 3])

Vector B: 
 tensor([4, 5, 6])

Dot product 
 tensor(32)
```

6. **Matrix Operations - Batch matrix multiplication**

```python
# Batch matrix multiplication
batch_A = torch.randn(3, 2, 4)    # 3 batches of 2x4 matrices
batch_B = torch.randn(3, 4, 3)    # 3 batches of 4x3 matrices
batch_matmul = torch.bmm(batch_A, batch_B)  # 3x2x3 result

print("Shape batch A: \n", batch_A.shape)
print("\nShape batch B: \n", batch_B.shape)
print("\nShape batch matmul: \n", batch_matmul.shape)
```

**Expected output**

```
Shape batch A: 
 torch.Size([3, 2, 4])

Shape batch B: 
 torch.Size([3, 4, 3])

Shape batch matmul: 
 torch.Size([3, 2, 3])
```

7. **Reduction Operations**

```python
tensor = torch.tensor([[1., 2., 3.], 
                       [4., 5., 6.]])

# Sum operations
sum_all = tensor.sum()            # 21
sum_dim0 = tensor.sum(dim=0)      # [5, 7, 9] (sum along rows)
sum_dim1 = tensor.sum(dim=1)      # [6, 15] (sum along columns)

# Mean operations
mean_all = tensor.mean()          # 3.5
mean_dim0 = tensor.mean(dim=0)    # [2.5, 3.5, 4.5]

# Min/Max operations
max_val, max_idx = tensor.max(dim=1)  # (values: [3,6], indices: [2,2])
min_val = tensor.min()            # 1

# Other reductions
prod = tensor.prod()              # Product: 720
std = tensor.std()                # Standard deviation
var = tensor.var()                # Variance

print("sum_all: ", sum_all)
print("sum_dim0: ", sum_dim0)
print("sum_dim1: ", sum_dim1)
print("mean_all: ", mean_all)
print("mean_dim0: ", mean_dim0)
print("max_val: ", mean_all, " And max_idx", max_idx)
print("min_val: ", min_val)
print("prod: ", prod)
print("Standard deviation: ", std)
print("Variance: ", var)
```

**Expected output**

```
sum_all:  tensor(21.)
sum_dim0:  tensor([5., 7., 9.])
sum_dim1:  tensor([ 6., 15.])
mean_all:  tensor(3.5000)
mean_dim0:  tensor([2.5000, 3.5000, 4.5000])
max_val:  tensor(3.5000)  And max_idx tensor([2, 2])
min_val:  tensor(1.)
prod:  tensor(720.)
Standard deviation:  tensor(1.8708)
Variance:  tensor(3.5000)
```
8. **Trigonometric Functions**

```python
angles = torch.tensor([0, torch.pi/4, torch.pi/2])

# Basic trig functions
sin_val = torch.sin(angles)       # [0., 0.7071, 1.]
cos_val = torch.cos(angles)       # [1., 0.7071, 0.]
tan_val = torch.tensor([0, 1, 2])
atan_val = torch.atan(tan_val)    # Inverse tangent

# Hyperbolic functions
sinh_val = torch.sinh(angles)
cosh_val = torch.cosh(angles)

print("sin", sin_val)
print("cos", cos_val)
print("tan", tan_val)
print("atan", atan_val)
print("sinh", sinh_val)
print("cosh", sinh_val)
```

**Expected output**

```
sin tensor([0.0000, 0.7071, 1.0000])
cos tensor([ 1.0000e+00,  7.0711e-01, -4.3711e-08])
tan tensor([0, 1, 2])
atan tensor([0.0000, 0.7854, 1.1071])
sinh tensor([0.0000, 0.8687, 2.3013])
cosh tensor([0.0000, 0.8687, 2.3013])
```
9. **Exponential and Logarithmic Functions**

```python
x = torch.tensor([1.0, 2.0, 3.0])

# Exponential functions
exp = torch.exp(x)                # e^x: [2.718, 7.389, 20.085]
exp2 = torch.exp2(x)              # 2^x: [2., 4., 8.]
pow = torch.pow(x, 2)             # x²: [1., 4., 9.]

# Logarithmic functions
log = torch.log(x)                # Natural log: [0., 0.693, 1.099]
log10 = torch.log10(x)            # Base-10 log: [0., 0.301, 0.477]
log2 = torch.log2(x)              # Base-2 log: [0., 1., 1.585]

# Special functions
sqrt = torch.sqrt(x)              # Square root: [1., 1.414, 1.732]
rsqrt = torch.rsqrt(x)            # Reciprocal square root

print("e^x:", exp)
print("2^x:", exp2)
print("x²:", pow)
print("Natural log:", log)
print("Base-10 log:", log10)
print("Base-2 log:", log2)
print("Square root:", sqrt)
print("Reciprocal square root:", rsqrt)
```

**Expected output**

```
e^x: tensor([ 2.7183,  7.3891, 20.0855])
2^x: tensor([2., 4., 8.])
x²: tensor([1., 4., 9.])
Natural log: tensor([0.0000, 0.6931, 1.0986])
Base-10 log: tensor([0.0000, 0.3010, 0.4771])
Base-2 log: tensor([0.0000, 1.0000, 1.5850])
Square root: tensor([1.0000, 1.4142, 1.7321])
Reciprocal square root: tensor([1.0000, 0.7071, 0.5774])
```
10. **Rounding Operations**

```python
x = torch.tensor([1.4, 2.6, -1.7, 3.0])

# Rounding operations
floor = torch.floor(x)            # [1., 2., -2., 3.]
ceil = torch.ceil(x)              # [2., 3., -1., 3.]
round_val = torch.round(x)        # [1., 3., -2., 3.]
trunc = torch.trunc(x)            # [1., 2., -1., 3.]

print("floor: ", floor)
print("ceil: ", ceil)
print("round_val: ", round_val)
print("trunc: ", trunc)
```

**Expected output**

```
floor:  tensor([ 1.,  2., -2.,  3.])
ceil:  tensor([ 2.,  3., -1.,  3.])
round_val:  tensor([ 1.,  3., -2.,  3.])
trunc:  tensor([ 1.,  2., -1.,  3.])
```
11. **Comparison Operations**

```python
# Comparison operations
a = torch.tensor([1, 2, 3])
b = torch.tensor([2, 2, 2])

equal = a == b                    # [False, True, False]
not_equal = a != b                # [True, False, True]
greater = a > b                   # [False, False, True]
less = a < b                      # [True, False, False]

# All/any operations
all_true = torch.all(a > 0)       # True (all elements > 0)
any_true = torch.any(a > 2)       # True (at least one element > 2)

print("equal: ", equal)
print("not_equal: ", not_equal)
print("greater: ", greater)
print("less: ", less)
print("True (all elements > 0): ", all_true)
print("True (at least one element > 2): ", any_true)
```

**Expected output**

```
equal:  tensor([False,  True, False])
not_equal:  tensor([ True, False,  True])
greater:  tensor([False, False,  True])
less:  tensor([ True, False, False])
True (all elements > 0):  tensor(True)
True (at least one element > 2):  tensor(True)
```

12. **Statistical Operations**

```python
tensor = torch.tensor([[1, 0, 12], 
                       [4, 3, 9], 
                       [2, 7, 6]])

# Statistical functions
mean = torch.mean(tensor.float(), dim=1) # 5.0
median = torch.median(tensor)     # 5
std = torch.std(tensor.float())   # Standard deviation
var = torch.var(tensor.float())   # Variance

# Sorting
sorted_vals, sorted_indices = torch.sort(tensor, dim=1)

# Top-k elements
topk_vals, topk_indices = torch.topk(tensor, k=2, dim=1)

print("Original tensor: \n", tensor)
print("mean: ", mean)
print("median: ", median)
print("Standard deviation: ", std)
print("Variance: ", var)
print("sorted_vals: \n", sorted_vals, "\nsorted_indices: \n", sorted_indices)
print("topk_vals: \n", topk_vals, "\ntopk_indices\n", topk_indices)

```

**Expected output**

```
Original tensor: 
 tensor([[ 1,  0, 12],
        [ 4,  3,  9],
        [ 2,  7,  6]])
mean:  tensor([4.3333, 5.3333, 5.0000])
median:  tensor(4)
Standard deviation:  tensor(3.9511)
Variance:  tensor(15.6111)
sorted_vals: 
 tensor([[ 0,  1, 12],
        [ 3,  4,  9],
        [ 2,  6,  7]]) 
sorted_indices: 
 tensor([[1, 0, 2],
        [1, 0, 2],
        [0, 2, 1]])
topk_vals: 
 tensor([[12,  1],
        [ 9,  4],
        [ 7,  6]]) 
topk_indices
 tensor([[2, 0],
        [2, 0],
        [1, 2]])
```

13. **Clamping and Thresholding**

```python
x = torch.tensor([-2, -1, 0, 1, 2, 3, 4])

# Clamping values
clamped_min = torch.clamp(x, min=0)    # [0, 0, 0, 1, 2, 3, 4]
clamped_max = torch.clamp(x, max=2)    # [-2, -1, 0, 1, 2, 2, 2]
clamped_range = torch.clamp(x, min=0, max=3)   # [0, 0, 0, 1, 2, 3, 3]

# Absolute value
abs_val = torch.abs(x)

# Sign function
sign = torch.sign(x)   # [-1, 0, 1]

print("Original vector: ", x)
print("clamped_min: ", clamped_min)
print("clamped_max: ", clamped_max)
print("clamped_range: ", clamped_range)
print("abs_val: ", abs_val)
print("sign: ", sign)
```

**Expected output**

```
Original vector:  tensor([-2, -1,  0,  1,  2,  3,  4])
clamped_min:  tensor([0, 0, 0, 1, 2, 3, 4])
clamped_max:  tensor([-2, -1,  0,  1,  2,  2,  2])
clamped_range:  tensor([0, 0, 0, 1, 2, 3, 3])
abs_val:  tensor([2, 1, 0, 1, 2, 3, 4])
sign:  tensor([-1, -1,  0,  1,  1,  1,  1])
```
14. **Broadcasting Operations**

```python
# Broadcasting allows operations between different shaped tensors
matrix = torch.tensor([[1, 2, 3],
                       [4, 5, 6]])  # 2x3

vector = torch.tensor([10, 20, 30]) # 3

# Broadcasting: vector is expanded to match matrix shape
result = matrix + vector          # [[11, 22, 33], [14, 25, 36]]

# Scalar broadcasting
scalar_result = matrix * 2        # [[2, 4, 6], [8, 10, 12]]

print("Broadcasting allows operations between different shaped tensors")
print("Original matrix: \n", matrix)
print("Original vector: \n", vector)

print("\nBroadcasting (matrix + vector): \n", result)
print("\nScalar broadcasting (matrix * 2 ): \n", scalar_result)
```

**Expected output**

```
Broadcasting allows operations between different shaped tensors
Original matrix: 
 tensor([[1, 2, 3],
        [4, 5, 6]])
Original vector: 
 tensor([10, 20, 30])

Broadcasting (matrix + vector): 
 tensor([[11, 22, 33],
        [14, 25, 36]])

Scalar broadcasting (matrix * 2 ): 
 tensor([[ 2,  4,  6],
        [ 8, 10, 12]])
```

## Casting Functions in PyTorch
1. **Basic Type Casting Methods**

```python
import torch

# Create a tensor
tensor = torch.tensor([-1, 0, 1, 2, 3, 4])
print("Original:", tensor, tensor.dtype)  # torch.int64

# Method 1: Using .to() method (recommended)
float_tensor = tensor.to(torch.float32)
double_tensor = tensor.to(torch.float64)
half_tensor = tensor.to(torch.float16)

print("to(float32):", float_tensor.dtype)    # torch.float32
print("to(float64):", double_tensor.dtype)   # torch.float64
print("to(float16):", half_tensor.dtype)     # torch.float16

# Method 2: Using type-specific methods
float_tensor2 = tensor.float()      # Convert to float32
double_tensor2 = tensor.double()    # Convert to float64
half_tensor2 = tensor.half()        # Convert to float16
int_tensor = tensor.int()           # Convert to int32
long_tensor = tensor.long()         # Convert to int64
float_to_bool = float_tensor.bool()      # non-zero → True)

print(".float():", float_tensor2.dtype)   # torch.float32
print(".int():", int_tensor.dtype)        # torch.int32
print(".float_to_bool: ", float_to_bool)
```

**Expected output**

```
Original: tensor([-1,  0,  1,  2,  3,  4]) torch.int64
to(float32): torch.float32
to(float64): torch.float64
to(float16): torch.float16
.float(): torch.float32
.int(): torch.int32
.float_to_bool:  tensor([ True, False,  True,  True,  True,  True])
```

2. **Using torch.as_tensor() for Casting**

```python
import torch
import numpy as np

# Convert with specific dtype
original = torch.tensor([1, 2, 3])
casted = torch.as_tensor(original, dtype=torch.float32)

print("Original:", original.dtype)  # torch.int64
print("Casted:", casted.dtype)      # torch.float32

# Works with various data sources
from_numpy = torch.as_tensor(np.array([1, 2, 3]), dtype=torch.float64)
from_list = torch.as_tensor([1.5, 2.5], dtype=torch.int32)

print("From numpy:", from_numpy.dtype)  # torch.float64
print("From list:", from_list.dtype)    # torch.int32 (truncates to [1, 2])
```

**Expected output**

```
Original: torch.int64
Casted: torch.float32
From numpy: torch.float64
From list: torch.int32
```

## Random Values in PyTorch
PyTorch provides comprehensive functions for generating random tensors with various distributions.

1. **Random Tensor - Uniform distribution**

```python
import torch

# Uniform distribution [0, 1)
uniform = torch.rand(2, 3)     # 2x3 tensor with values between 0 and 1
print("Uniform [0,1):")
print(uniform)
print("Shape:", uniform.shape)
```

**Expected output**

```
Uniform [0,1):
tensor([[0.5321, 0.6660, 0.3010],
        [0.0778, 0.6902, 0.1458]])
Shape: torch.Size([2, 3])
```

2. **Random Tensor - Normal distribution (mean=0, std=1)**

```python
import torch

# Normal distribution (mean=0, std=1)
normal = torch.randn(2, 3)  # 2x3 tensor from normal distribution
print("\nNormal (0,1):")
print(normal)
```

**Expected output**

```
Normal (0,1):
tensor([[ 0.5853,  1.1133,  0.4959],
        [-0.7744,  0.8367,  1.4931]])
```

3. **Random Tensor - Integer random values**

```python
import torch

# Integer random values
integers = torch.randint(0, 10, (2, 3))  # 2x3 tensor, values 0-9
print("\nIntegers [0,10):")
print(integers)
```

**Expected output**

```
Integers [0,10):
tensor([[5, 1, 2],
        [6, 3, 1]])
```

4. **Random Tensor - Specific range uniform**

```python
# Specific range uniform
rand_range = torch.FloatTensor(3, 3).uniform_(-5, 5)  # Uniform [-5, 5]
print("\nUniform [-5,5]:\n", rand_range)
```

**Expected output**

```
Uniform [-5,5]:
 tensor([[-2.6929,  2.9214,  0.6373],
        [ 1.0963, -2.6829, -0.9829],
        [ 0.3103,  2.0133,  3.5047]])
```

5. **Advanced Random Distributions**

```python
# Gamma distribution
gamma = torch.distributions.Gamma(2.0, 1.0).sample((3,))
print("Gamma samples:", gamma)

# Beta distribution
beta = torch.distributions.Beta(2.0, 5.0).sample((3,))
print("Beta samples:", beta)

# Poisson distribution
poisson = torch.poisson(torch.tensor([1.0, 2.0, 3.0]))
print("Poisson samples:", poisson)

# Exponential distribution
exponential = torch.distributions.Exponential(1.0).sample((3,))
print("Exponential samples:", exponential)
```

**Expected output**

```
Gamma samples: tensor([2.0656, 2.7432, 1.9295])
Beta samples: tensor([0.2216, 0.5546, 0.2412])
Poisson samples: tensor([0., 1., 5.])
Exponential samples: tensor([0.2998, 1.5631, 0.3579])
```

6. **In-place Random Operations**

```python
# Create a tensor
tensor = torch.zeros(3, 3)

# Fill with random values in-place
tensor.uniform_()           # Fill with uniform [0,1)
print("After uniform_():\n", tensor)

tensor.normal_()            # Fill with normal (0,1)
print("\nAfter normal_():\n", tensor)

tensor.bernoulli_(0.5)      # Fill with Bernoulli (p=0.5)
print("\nAfter bernoulli_(0.5):\n", tensor)

# Specific range
tensor.uniform_(-1, 1)      # Fill with uniform [-1,1)
print("\nAfter uniform_(-1,1):\n", tensor)
```

**Expected output**

```
After uniform_():
 tensor([[0.0740, 0.8665, 0.1366],
        [0.1025, 0.1841, 0.7264],
        [0.3153, 0.6871, 0.0756]])

After normal_():
 tensor([[-0.6219, -0.3076, -0.5987],
        [-0.5564, -0.0596, -1.9858],
        [-0.2109,  1.9667, -0.8350]])

After bernoulli_(0.5):
 tensor([[0., 0., 1.],
        [0., 0., 1.],
        [0., 0., 0.]])

After uniform_(-1,1):
 tensor([[ 0.8313, -0.1320, -0.8457],
        [-0.2869, -0.7043,  0.0661],
        [-0.1867, -0.5364, -0.0909]])
```

## Seed in PyTorch
Seeding in PyTorch is crucial for reproducibility in machine learning experiments. 

1. **Basic Seeding Methods - Set seed for CPU operations**

```python
import torch
import numpy as np

# Set seed for CPU operations
torch.manual_seed(42)

# Generate random tensors
random1 = torch.rand(2, 3)
random2 = torch.randn(2, 3)

print("With seed 42:")
print("Random 1:\n", random1)
print("Random 2:\n", random2)

# Reset seed to get same sequence
torch.manual_seed(42)
random1_again = torch.rand(2, 3)
random2_again = torch.randn(2, 3)

print("\nSame seed again:")
print("Random 1 again:\n", random1_again)
print("Random 2 again:\n", random2_again)

print("\nAre they equal?")
print("Random1:", torch.equal(random1, random1_again))
print("Random2:", torch.equal(random2, random2_again))
```

**Expected output**

```
With seed 42:
Random 1:
 tensor([[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009]])
Random 2:
 tensor([[ 1.1561,  0.3965, -2.4661],
        [ 0.3623,  0.3765, -0.1808]])

Same seed again:
Random 1 again:
 tensor([[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009]])
Random 2 again:
 tensor([[ 1.1561,  0.3965, -2.4661],
        [ 0.3623,  0.3765, -0.1808]])

Are they equal?
Random1: True
Random2: True
```
2. **Basic Seeding Methods - Set seed for GPU operations**

```python
# Set seed for GPU operations
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)  # For multi-GPU
    
    # Generate random tensors on GPU
    gpu_random = torch.rand(2, 3, device='cuda')
    print("GPU random tensor:\n", gpu_random.cpu())  # Move to CPU for printing
```

3. **PyTorch with seed(123)**

A **seed** is like a "starting code" that makes random number generation **reproducible**. Think of it as giving the same instructions to a magic dice - every time you use the same seed, you'll get the exact same sequence of "random" numbers. This is crucial in programming and machine learning because it ensures that your experiments can be reproduced exactly, making your results reliable and verifiable.

* **Without seed**: "Shuffle the cards" → Different result every time.
* **With seed**: "Shuffle the cards using method X, starting with card Y" → Same result every time

**The specific number** `123` **is arbitrary - you can use any integer such as** `999`. The important thing is to use the same seed value consistently across all components of your project for true reproducibility.


```python
import torch

# Set PyTorch seed to 123
torch.manual_seed(123)

# Generate reproducible random tensors
random_tensor = torch.rand(3, 3)
random_normal = torch.randn(5)
random_ints = torch.randint(0, 10, (5,))

print("PyTorch with seed 123:")
print("Random tensor:\n", random_tensor)
print("Random normal:", random_normal)
print("Random integers:", random_ints)
```

**Expected output**

```
PyTorch with seed 123:
Random tensor:
 tensor([[0.2961, 0.5166, 0.2517],
        [0.6886, 0.0740, 0.8665],
        [0.1366, 0.1025, 0.1841]])
Random normal: tensor([ 0.4422, -1.0854, -0.6219, -0.3076, -0.5987])
Random integers: tensor([6, 5, 7, 9, 2])
```

The only difference between `seed(123)` and `seed(42)` is the **starting point** they provide to the random number generator. Both work exactly the same way, but they **produce completely different sequences** of random numbers. **The only rule**:
1. **Choose one seed value**.
2. Use it **consistently** throughout your project.
3. Document which seed you used.

## Indices of the Maximum Values
`argmax()` returns the indices of the maximum values along a specified dimension. It's essential for finding the most likely class in classification tasks.

1. **Basic Usage**

```python
import torch

# 1D tensor
tensor_1d = torch.tensor([3, 1, 4, 1, 5, 9, 2])
max_index = torch.argmax(tensor_1d)
print("1D tensor:", tensor_1d)
print("argmax:", max_index.item())  
print("Max value:", tensor_1d[max_index].item())
```

**Expected output**

```
1D tensor: tensor([3, 1, 4, 1, 5, 9, 2])
argmax: 5
Max value: 9
```
2. **Along Different Dimensions**

```python
# 2D tensor
tensor_2d = torch.tensor([
    [1, 5, 3],
    [4, 2, 6], 
    [7, 8, 0]
])
print("2D tensor:\n", tensor_2d)

# Default: flatten and find global maximum
global_max = torch.argmax(tensor_2d)
print("Global argmax (flattened):", global_max.item())  # 7 (index in flattened view)

# Along rows (dim=0) - find max in each column
max_per_column = torch.argmax(tensor_2d, dim=0)
print("argmax along dim=0 (columns):", max_per_column)  # [2, 2, 1]

# Along columns (dim=1) - find max in each row  
max_per_row = torch.argmax(tensor_2d, dim=1)
print("argmax along dim=1 (rows):", max_per_row)  # [1, 2, 1]
```

**Expected output**

```
2D tensor:
 tensor([[1, 5, 3],
        [4, 2, 6],
        [7, 8, 0]])
Global argmax (flattened): 7
argmax along dim=0 (columns): tensor([2, 2, 1])
argmax along dim=1 (rows): tensor([1, 2, 1])
```

## Indices of the Minimum Values
`argmin()` returns the indices of the maximum values along a specified dimension. It's essential for finding the most likely class in classification tasks.

1. **Basic Usage**

```python
import torch

# 1D tensor
tensor_1d = torch.tensor([3, 1, 4, 1, 5, 9, 2])
min_index = torch.argmin(tensor_1d)
print("1D tensor:", tensor_1d)
print("argmin:", min_index.item())  
print("Min value:", tensor_1d[min_index].item())
```

**Expected output**

```
1D tensor: tensor([3, 1, 4, 1, 5, 9, 2])
argmin: 1
Min value: 1
```
2. **Along Different Dimensions**

```python
# 2D tensor
tensor_2d = torch.tensor([
    [5, 1, 3],
    [4, 2, 6], 
    [7, 8, 0]
])
print("2D tensor:\n", tensor_2d)

# Default: flatten and find global maximum
global_min = torch.argmin(tensor_2d)
print("Global argmin (flattened):", global_min.item())  

# Along rows (dim=0) - find max in each column
min_per_column = torch.argmin(tensor_2d, dim=0)
print("argmin along dim=0 (columns):", min_per_column)  

# Along columns (dim=1) - find max in each row  
min_per_row = torch.argmin(tensor_2d, dim=1)
print("argmin along dim=1 (rows):", min_per_row) 
```

**Expected output**

```
2D tensor:
 tensor([[5, 1, 3],
        [4, 2, 6],
        [7, 8, 0]])
Global argmin (flattened): 8
argmin along dim=0 (columns): tensor([1, 0, 2])
argmin along dim=1 (rows): tensor([1, 1, 2])
```

## One-Hot Encoding in PyTorch
One-hot encoding converts categorical labels into binary vectors where only one element is "hot" (1) and all others are "cold" (0). PyTorch provides `torch.nn.functional.one_hot()` for this purpose.

1. **Basic One-Hot Encoding**

```python
import torch
import torch.nn.functional as F

# Multiple labels
labels = torch.tensor([0, 2, 1, 3])
one_hot_batch = F.one_hot(labels, num_classes=4)
print("\nLabels:", labels)
print("One-hot batch:\n", one_hot_batch)
```

**Expected output**

```
Labels: tensor([0, 2, 1, 3])
One-hot batch:
 tensor([[1, 0, 0, 0],
        [0, 0, 1, 0],
        [0, 1, 0, 0],
        [0, 0, 0, 1]])

```

2. **Multi-dimensional Tensors**

```python
# 2D tensor of labels
labels_2d = torch.tensor([[0, 1], 
                          [2, 0],
                          [1, 2]])
print("2D labels:\n", labels_2d)

# One-hot encoding adds a new dimension
one_hot_2d = F.one_hot(labels_2d, num_classes=3)
print("\n2D one-hot shape:", one_hot_2d.shape)  # torch.Size([3, 2, 3])
print("2D one-hot:\n", one_hot_2d)
```

**Expected output**

```
2D labels:
 tensor([[0, 1],
        [2, 0],
        [1, 2]])

2D one-hot shape: torch.Size([3, 2, 3])
2D one-hot:
 tensor([[[1, 0, 0],
         [0, 1, 0]],

        [[0, 0, 1],
         [1, 0, 0]],

        [[0, 1, 0],
         [0, 0, 1]]])
```
