# Introduction to Pytorch

PyTorch is an open-source, **Python-based deep learning library** widely used for both research and production. According to Papers With Code, PyTorch has been the most widely used deep learning framework in research since 2019. Furthermore, the Kaggle Data Science and Machine Learning Survey 2022 reported that around $40%$ of respondents use PyTorch and this rate continues to grow each year. Reasons why PyTorch is popular:
1. **User-friendly interface**: Easy to learn and use
2. **Efficiency**: Optimized performance
3. **Flexibility**: Allows low-level customization while maintaining accessibility
4. **Balanced design**: Perfect blend of usability and advanced features

### The Three Core Components
PyTorch can be understood in terms of its three main components.
1. **Tensor Library**
   * **Foundation**: Extends NumPy's array-oriented programming.
   * **Key advantage**: Seamless GPU acceleration while maintaining CPU compatibility.
   * **Purpose**: Fundamental building block for all computations.
     
2. **Automatic Differentiation (autograd)**
   * **Function**: Automatically computes gradients for tensor operations.
   * **Benefit**: Simplifies backpropagation and model optimization.
   * **Impact**: Eliminates manual gradient calculation, making neural network training much easier.

3. **Deep Learning Library**
   * **Features**: Modular building blocks, pretrained models, loss functions, optimizers.
   * **Design philosophy**: Flexible and efficient components.
   * **Audience**: Caters to both researchers (flexibility) and developers (ease of use)

## Artificial Intelligence (AI)
PyTorch is a deep learning library used to build and train AI models such as LLM and CNN. For instance, Large Language Models (LLMs) are often referred to as AI models. However, LLMs are also a type of deep neural network, and PyTorch is a deep learning library used to build and train such models. This overlap in terminology can be confusing, so let’s briefly summarize the relationship between these terms before proceeding (the AI hierarchy):

1. **Artificial Intelligence (AI)**
   * Systems that perform tasks requiring human intelligence.
   * **Examples**: Natural language understanding, pattern recognition, decision making.
     
2. **Machine Learning (ML) - Subset of AI**
   * Algorithms that learn from data without explicit programming.
   * Improve performance over time with more data and feedback.
   * Real-world applications:
     - Recommendation systems (Netflix, Amazon)
     - Email spam filtering
     - Voice recognition (Siri, Alexa)
     - Self-driving cars
       
3. **Deep Learning (DL) - Subset of Machine Learning**
   * Deep neural networks with multiple hidden layers
   * Loosely modeled after the human brain's neural connections
   * Excellent at handling unstructured data (images, audio, text)
   * Multiple layers enabling complex, nonlinear relationships
   * Large Language Models (LLM) are a type of Deep Neural Network, which makes them part of this hierarchy.

    <img width="313" height="314" alt="image" src="https://github.com/user-attachments/assets/a05dfa4a-754e-455a-a1fc-738cf449bfaf" />


 ### Supervised and Unsupervised Learning
 In **machine learning**, algorithms are often categorized based on the type of data available during training and the nature of the learning objective. Two of the most common paradigms are **supervised learning** and **unsupervised learning**. Each serves different purposes and is suited for different types of tasks.

1. **Supervised Learning**
   * Supervised learning involves training a model on a dataset that contains both **inputs (features)** and their corresponding **outputs (labels)**.
   * The model learns to map inputs to outputs by minimizing the difference between its predictions and the true labels.
   * **Common Tasks:**
     - **Classification**: Predict discrete categories (spam/not spam, cat/dog)
     - **Regression**: Predict continuous values (house prices, temperature)
       
   * **Example**
     - In an email spam classifier, each training sample (email) has a label—either “spam” or “not spam.”
     - LLM pretraining via next-word prediction (sequence → next word)
     - In image classification, the input might be an image, and the label could be a class such as “cat”, “dog”, or “car.”
       
   * **Common algorithms and models include:**
     - Linear regression and logistic regression
     - Decision trees and random forests
     - Support vector machines (SVMs)
     - Neural networks and deep learning architectures (e.g., CNNs, LSTMs, Transformers)

2. **Unsupervised Learning**
   * Unsupervised learning deals with data that does not contain labels.
   * The algorithm explores the structure of the data on its own to find patterns, relationships, or groupings among examples.
   * The goal is to uncover hidden structure rather than predict explicit outcomes.
   * Unlike supervised learning, where feedback is provided in the form of correct labels, unsupervised learning relies on data similarity and statistical relationships to discover structure.
     
   * **Common Tasks:**
     - **Clustering**: Group similar data points (customer segmentation)
     - **Anomaly Detection**: Find unusual data points
     - **Association: Discover** relationships between variables
       
   * **Examples:**
     - Grouping news articles by topic without knowing categories
     - Finding similar customers based on purchasing behavior
     - Reducing image dimensions while preserving important features
       
   * **Common algorithms**
     - K-means, hierarchical clustering, DBSCAN, ...

While supervised and unsupervised learning represent two ends of the machine learning spectrum, many real-world problems **fall somewhere in between**. In practice, **labeled data is often scarce** or expensive to obtain, while unlabeled data is abundant. This challenge has led to the development of **semi-supervised** and **self-supervised learning** techniques, both of which aim to make better use of available unlabeled data.

3. **Semi-supervised Learning**
   * Semi-supervised learning combines aspects of both supervised and unsupervised learning.
   * It leverages a small amount of labeled data together with a large amount of unlabeled data to improve model performance.
   * The key idea is that the model first learns the general structure or distribution of the data from the unlabeled portion and then refines its understanding using the limited labeled examples.
     
    * **Real-world Example:**
      - Medical imaging: 100 labeled tumor scans + 10,000 unlabeled scans
      - Text classification: 1,000 labeled customer reviews + 1 million unlabeled reviews
    * Semi-supervised learning has proven particularly effective when obtaining labeled data is costly or time-consuming, such as in healthcare, fraud detection, and natural language processing (NLP).
    * **Common techniques**
      - Self-training: Model labels its own predictions on unlabeled data, then retrains
      - Co-training: Multiple models teach each other using different data views

4. **Self-supervised Learning**
   * **Core Idea**: Generate labels automatically from the data itself.
   * **No human labels**: The data provides its own supervision signal
   * **Examples:**
     - Predict missing words in sentences
     - Next sentence prediction
     - Recover missing parts of images



| Learning Type       | Data Requirement                       | Uses Labels          | Example Tasks                        | Typical Use Case                        |
| ------------------- | -------------------------------------- | -------------------- | ------------------------------------ | --------------------------------------- |
| **Supervised**      | Labeled data only                      | ✅ Yes                | Classification, regression           | Spam detection, object recognition      |
| **Unsupervised**    | Unlabeled data only                    | ❌ No                 | Clustering, dimensionality reduction | Customer segmentation, topic modeling   |
| **Semi-supervised** | Mix of labeled + unlabeled             | ⚙️ Partially         | Classification with limited labels   | Medical imaging, fraud detection        |
| **Self-supervised** | Unlabeled data (auto-generated labels) | ⚙️ Derived from data | Representation learning, pretraining | LLMs, vision transformers, autoencoders |

## Installing PyTorch
**PyTorch** is a large, flexible deep learning library with both **CPU** and **GPU** support. Because of that, installation depends on your system’s hardware and Python environment.

1. **Python Version Compatibility**
   * Use a stable version that’s one or two releases behind the newest one.
   * For example, if the latest is Python 3.13, install Python 3.11 or 3.12 for best compatibility.
   * Reason: Scientific libraries take time to support newest Python versions
     
  2. **Basic CPU-only installation**
     * If you don’t have a GPU or don’t need GPU acceleration:

       ```
          pip install torch
       ```
     * Installing with **CUDA for NVIDIA GPUs
        - Go to https://pytorch.org
        - Use the installation selector to get the correct command for your OS and CUDA version.
        - Example (for PyTorch 2.4.0):

       ```
          pip install torch==2.4.0 torchvision torchaudio
       ```
  3. **Verify installation**
       * Run this in Python:
    
       ```
          import torch
          print(torch.__version__)
       ```
       * Expected output:
    
       ```
          '2.4.0'
       ```
       
4. **Check GPU availability**
   * To verify GPU support:

       ```
          import torch
          print(torch.cuda.is_available())
       ```
   * If it prints True, GPU acceleration is active.
   * If it prints False, check your GPU drivers, CUDA, or reinstall with the right CUDA version.
  
5. **Apple Silicon (M1/M2/M3) acceleration**
   * If using a Mac with Apple Silicon:

       ```
          import torch
          print(torch.backends.mps.is_available())
       ```
   * If it prints True, PyTorch can use the Metal Performance Shaders (MPS) backend for acceleration.

6. **Using Google Colab (no GPU at home)**
   * If you don’t have a GPU locally:
     - Open https://colab.research.google.com
     - Go to **Runtime → Change runtime type → Hardware accelerator → GPU**
     - Then run your PyTorch code with GPU support for free (time-limited).

       <img width="383" height="319" alt="image" src="https://github.com/user-attachments/assets/19330dcc-8b7b-451e-930a-ceff0e053fad" />

       
 7. **Common Techniques for Installing and Setting Up PyTorch**
    * Use **virtual environments** (e.g., venv or conda) to isolate dependencies.
    * **Match CUDA version** with your NVIDIA driver.
    * Always **test GPU availability** with torch.cuda.is_available().
    * **Pin library versions** for reproducibility (torch==2.4.0).
    * Use **Colab or cloud GPUs** if your machine lacks one.
    * **Update pip** before installation to avoid dependency errors:

       ```
          pip install --upgrade pip
       ```

## Tensor Creation and Data Types
**Tensors are the fundamental data structure** used in PyTorch and all modern deep learning frameworks.
They generalize mathematical structures like **scalars, vectors, and matrixes** to higher dimensions.

A tensor is a** multidimensional array** — a mathematical object that can be described by its **rank (order)**, which indicates **how many dimensions (or axes)** it has.

| Tensor Type | Rank (Dimensions) | Example                          | Shape       |
| ----------- | ----------------- | -------------------------------- | ----------- |
| Scalar      | 0D                | `5`                              | `()`        |
| Vector      | 1D                | `[1, 2, 3]`                      | `(3,)`      |
| Matrix      | 2D                | `[[1, 2], [3, 4]]`               | `(2, 2)`    |
| 3D Tensor   | 3D                | `[[[1,2],[3,4]], [[5,6],[7,8]]]` | `(2, 2, 2)` |

**Interpretation:**
1. **Scalars (0D)** store single values.
2. **Vectors (1D)** store ordered lists of numbers.
3. **Matrixes (2D)** represent grids of numbers.
4. **Higher-rank tensors (e.g 3D)** represent multi-dimensional data such as images, sequences, or batches.

### Computational Perspective
1. **Data Containers**
   * From a computing viewpoint, **tensors are containers for numerical data**.
   * Each dimension corresponds to a **feature or property** of that data.
   * Essential for **representing complex data** like images, text, etc.
   * **Examples:**
     - **A 1D tensor** → a list of temperatures over time.
     - **A 2D tensor** → a grayscale image (height × width).
     - **A 3D tensor** → an RGB image (height × width × 3 channels).
     - **A 4D tensor** → a batch of RGB images.

2. **PyTorch vs NumPy Comparison**
   * **Similarities:**
     - Similar API and syntax
     - Support similar operations (indexing, slicing, mathematical ops)
     - Can often convert between them easily
   * **PyTorch Advantages:**
     - **GPU acceleration** - Allow execution on **CPUs** and **GPUs**,
     - **Automatic differentiation** - Enable automatic differentiation for deep learning.
     - **Better for deep learning** - Optimized for neural networks
     - Support efficient mathematical computations

### Practical Tensor Examples: Scalars, Vectors, Matrixes, and Tensors
PyTorch tensors are **created using** the `torch.tensor()` function. Each rank of a tensor corresponds to how nested your data structure is.

```python
import torch

tensor0d = torch.tensor(1)                   # 0D tensor (scalar)
tensor1d = torch.tensor([1, 2, 3])           # 1D tensor (vector)
tensor2d = torch.tensor([[1, 2],
                         [3, 4]])            # 2D tensor (matrix)
tensor3d = torch.tensor([[[1, 2], [3, 4]],
                         [[5, 6], [7, 8]]])  # 3D tensor

print(tensor0d)
print(tensor1d)
print(tensor2d)
print(tensor3d)
```

Expected output:

```
tensor(1)
tensor([1, 2, 3])
tensor([[1, 2],
        [3, 4]])
tensor([[[1, 2],
         [3, 4]],

        [[5, 6],
         [7, 8]]])
```

### Creating Tensors with Float Data Type in PyTorch
There are several ways to create tensors with float data types in PyTorch. Here's a comprehensive guide:

**For most deep learning applications, use float32**

**Explicitly Specify dtype during Creation**

```python
import torch

# Method 1: Using dtype parameter
float_tensor1 = torch.tensor([1, 2, 3, 4], dtype=torch.float32)
float_tensor2 = torch.tensor([1.5, 2.7, 3.9], dtype=torch.float64)

print("Float32 tensor:", float_tensor1, float_tensor1.dtype)
print("Float64 tensor:", float_tensor2, float_tensor2.dtype)
```

**Expected output**

```
Float32 tensor: tensor([1., 2., 3., 4.]) torch.float32
Float64 tensor: tensor([1.5000, 2.7000, 3.9000], dtype=torch.float64) torch.float64
```

**Using Python Float Literals**

```python
# PyTorch automatically infers float32 from Python floats
auto_float = torch.tensor([1.0, 2.0, 3.0])  # Automatically float32
mixed = torch.tensor([1, 2.5, 3])           # Mixed -> promotes to float32

print("Auto float:", auto_float.dtype)      # torch.float32
print("Mixed types:", mixed.dtype)          # torch.float32
```

**Expected output**

```
Auto float: torch.float32
Mixed types: torch.float32
```

### Properties of a Tensor
A tensor in PyTorch has several important properties that describe its structure and contents:

| **Property**  | **Method / Attribute** | **Description**                                        | **Example Output**   |
| ------------- | ---------------------- | ------------------------------------------------------ | -------------------- |
| **Shape**     | `.shape`               | Size (length) of each dimension                        | `torch.Size([2, 3])` |
| **Rank**      | `.dim()`               | Number of dimensions (axes)                            | `2`                  |
| **Size**      | `.size()`              | Alias for `.shape`, gives tensor dimensions            | `torch.Size([2, 3])` |
| **Elements**  | `.numel()`             | Total number of elements in the tensor                 | `6`                  |
| **Data Type** | `.dtype`               | Type of data stored in tensor elements                 | `torch.float32`      |
| **Device**    | `.device`              | Indicates where the tensor is stored (`cpu` or `cuda`) | `device(type='cpu')` |


**For example consider the following code**

```python
import torch

# Example 1: 1D Tensor (Vector)
vector = torch.tensor([1, 2, 3, 4, 5])
print("=== 1D Tensor ===")
print("Tensor:", vector)
print("Shape:", vector.shape)                 # torch.Size([5])
print("Rank:", vector.dim())                  # 1
print("Size:", vector.size())                 # torch.Size([5])
print("Number of elements:", vector.numel())  # 5
print("Data type: ", vector.dtype)            # torch.int64
print("Device: ", vector.device)              # cpu


# Example 2: 2D Tensor (Matrix)
matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])
print("\n=== 2D Tensor ===")
print("Tensor:", matrix)
print("Shape:", matrix.shape)                # torch.Size([2, 3])
print("Rank:", matrix.dim())                 # 2
print("Size:", matrix.size())                # torch.Size([2, 3])
print("Number of elements:", matrix.numel()) # 6
print("Data type: ", matrix.dtype)           # torch.int64
print("Device: ", matrix.device)              # cpu

# Example 3: 3D Tensor
tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
print("\n=== 3D Tensor ===")
print("Tensor:", tensor_3d)
print("Shape:", tensor_3d.shape)                # torch.Size([2, 2, 2])
print("Rank:", tensor_3d.dim())                 # 3
print("Size:", tensor_3d.size())                # torch.Size([2, 2, 2])
print("Number of elements:", tensor_3d.numel()) # 8
print("Data type: ", tensor_3d.dtype)              # torch.int64
print("Device: ", tensor_3d.device)              # cpu
```

**Expected output

```
=== 1D Tensor ===
Tensor: tensor([1, 2, 3, 4, 5])
Shape: torch.Size([5])
Rank: 1
Size: torch.Size([5])
Number of elements: 5
Data type:  torch.int64
Device:  cpu

=== 2D Tensor ===
Tensor: tensor([[1, 2, 3],
        [4, 5, 6]])
Shape: torch.Size([2, 3])
Rank: 2
Size: torch.Size([2, 3])
Number of elements: 6
Data type:  torch.int64
Device:  cpu

=== 3D Tensor ===
Tensor: tensor([[[1, 2],
         [3, 4]],

        [[5, 6],
         [7, 8]]])
Shape: torch.Size([2, 2, 2])
Rank: 3
Size: torch.Size([2, 2, 2])
Number of elements: 8
Data type:  torch.int64
Device:  cpu
```

## Tensor Indexing and Element Access 
Tensor indexing in PyTorch allows you to access and manipulate specific elements or subsets of your data with intuitive, NumPy-like syntax. Think of tensors as multi-dimensional containers where each dimension can be accessed using square brackets `[]`.
1. For 1D tensors (vectors), use single indices like `tensor[0]` to get the first element.
2. For 2D tensors (matrices), use comma-separated indices like `tensor[1, 2]` to access the element at row 1, column 2.
3. Slicing with colons: `tensor[:3]` gets the first three elements.
4. `tensor[:, 1]` gets the entire second column, and `tensor[::2]` gets every other element.
5. For more advanced selection, boolean masking lets you filter elements based on conditions (`tensor[tensor > 5]`), while integer array indexing allows picking specific indices.

**Examples:**

1. **1D tensor**

```python
import torch

# 1D tensor
vector = torch.tensor([10, 20, 30, 40, 50])
print("Original vector:", vector)

# Access single element
print("vector[2]:", vector[2])        # tensor(30)
print("vector[-1]:", vector[-1])      # tensor(50) - last element

# Access range of elements (slicing)
print("vector[1:4]:", vector[1:4])    # tensor([20, 30, 40])
print("vector[:3]:", vector[:3])      # tensor([10, 20, 30]) - first 3
print("vector[2:]:", vector[2:])      # tensor([30, 40, 50]) - from index 2
```

**Expected output:**
```
Original vector: tensor([10, 20, 30, 40, 50])
vector[2]: tensor(30)
vector[-1]: tensor(50)
vector[1:4]: tensor([20, 30, 40])
vector[:3]: tensor([10, 20, 30])
vector[2:]: tensor([30, 40, 50])
```

2. **2D Tensor**

```python
# 2D tensor (matrix)
matrix = torch.tensor([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9]])
print("Original matrix:")
print(matrix)

# Access single element
print("matrix[0, 1]:", matrix[0, 1])      # tensor(2) - row 0, column 1
print("matrix[2, 0]:", matrix[2, 0])      # tensor(7) - row 2, column 0

# Access entire row
print("matrix[1]:", matrix[1])            # tensor([4, 5, 6]) - row 1
print("matrix[1, :]:", matrix[1, :])      # Same as above

# Access entire column
print("matrix[:, 2]:", matrix[:, 2])      # tensor([3, 6, 9]) - column 2
```

**Expected output**

```
Original matrix:
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
matrix[0, 1]: tensor(2)
matrix[2, 0]: tensor(7)
matrix[1]: tensor([4, 5, 6])
matrix[1, :]: tensor([4, 5, 6])
matrix[:, 2]: tensor([3, 6, 9])
```

3. **3D tensor**

```python
# 3D tensor (batch, rows, columns)
tensor_3d = torch.tensor([[[1, 2], [3, 4]],
                          [[5, 6], [7, 8]],
                          [[9, 10], [11, 12]]])
print("3D tensor shape:", tensor_3d.shape)          # torch.Size([3, 2, 2])

# Access different dimensions
print("First batch:", tensor_3d[0])                 # tensor([[1, 2], [3, 4]])
print("Second batch, first row:", tensor_3d[1, 0])  # tensor([5, 6])
print("All batches, first row:", tensor_3d[:, 0])   # tensor([[1, 2], [5, 6], [9, 10]])
print("Specific element:", tensor_3d[2, 1, 0])      # tensor(11)
```

**Expected output**

```
3D tensor shape: torch.Size([3, 2, 2])
First batch: tensor([[1, 2],
        [3, 4]])
Second batch, first row: tensor([5, 6])
All batches, first row: tensor([[ 1,  2],
        [ 5,  6],
        [ 9, 10]])
Specific element: tensor(11)
```
4. **Boolean Masking**

```python
# Create a tensor
data = torch.tensor([5, 12, 8, 3, 15, 1])

# Boolean masking
mask = data > 7
print("Mask:", mask)                            # tensor([False, True, True, False, True, False])
print("Values > 7:", data[mask])                # tensor([12, 8, 15])

# Multiple conditions
mask2 = (data > 5) & (data < 10)
print("Values between 5 and 10:", data[mask2])  # tensor([8])

# Direct condition
print("Even numbers:", data[data % 2 == 0])     # tensor([12, 8])
```

**Expected output**

```
Mask: tensor([False,  True,  True, False,  True, False])
Values > 7: tensor([12,  8, 15])
Values between 5 and 10: tensor([8])
Even numbers: tensor([12,  8])
```
5. **Integer Array Indexing**

```python
tensor = torch.tensor([[10, 20, 30],
                       [40, 50, 60],
                       [70, 80, 90]])

# Select specific indices
print("Rows [0, 2], all columns:", tensor[[0, 2]])      # Rows 0 and 2, all columns
print("Columns [1, 2] of all rows:", tensor[:, [1, 2]]) # All rows, columns 1 and 2

# Advanced indexing - select specific elements
rows = torch.tensor([0, 1, 2])
cols = torch.tensor([1, 0, 2])

# From row o, select column 1 and From row 1, select column 0 and From row 2, select column 2
print("Specific elements:", tensor[rows, cols])         # [20, 40, 90]
```

**Expected output**

```
Rows [0, 2], all columns: tensor([[10, 20, 30],
        [70, 80, 90]])
Columns [1, 2] of all rows: tensor([[20, 30],
        [50, 60],
        [80, 90]])
Specific elements: tensor([20, 40, 90])
```

### Modifying Tensor Values in PyTorch
There are several ways to modify tensor values in PyTorch. Here's a comprehensive guide:
1. **Direct Assignment by Index - Single Element Modification**

```python
import torch

# Create a tensor
tensor = torch.tensor([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9]])
print("Original tensor:")
print(tensor)

# Modify single element
tensor[0, 1] = 99
print("\nAfter modifying [0,1]:")
print(tensor)
```
**Expected output**

```
Original tensor:
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])

After modifying [0,1]:
tensor([[ 1, 99,  3],
        [ 4,  5,  6],
        [ 7,  8,  9]])
```

2. **Direct Assignment by Index - Row/Column Modification**

```python
# Modify entire row
tensor[1] = torch.tensor([10, 20, 30])
print("\nAfter modifying row 1:")
print(tensor)

# Modify entire column
tensor[:, 2] = torch.tensor([100, 200, 300])
print("\nAfter modifying column 2:")
print(tensor)

# Modify entire column with a unique number
tensor[:, 1] = torch.tensor(-1)
print("\nAfter modifying column 1:")
print(tensor)
```

**Expected output**

```
After modifying row 1:
tensor([[ 1, 99, -1],
        [10, 20, 30],
        [ 7,  8, -1]])

After modifying column 2:
tensor([[  1,  99, 100],
        [ 10,  20, 200],
        [  7,   8, 300]])

After modifying column 1:
tensor([[  1,  -1, 100],
        [ 10,  -1, 200],
        [  7,  -1, 300]])
```
3. **Slicing and Range Modification**

```python
# Create new tensor
tensor = torch.tensor([[1, 2, 3, 4],
                       [5, 6, 7, 8],
                       [9, 10, 11, 12]])

# Modify a slice
tensor[0:2, 1:3] = torch.tensor([[20, 30], [60, 70]])
print("After slice modification:")
print(tensor)
```

**Output**

```
After slice modification:
tensor([[ 1, 20, 30,  4],
        [ 5, 60, 70,  8],
        [ 9, 10, 11, 12]])
```

4. **Boolean Masking Modification** 

```python
tensor = torch.tensor([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9]])

# Modify elements based on condition
print("\n============ Modify elements based on condition ==========\n")
mask = tensor > 5
tensor[mask] = -100
print("After setting values > 5 to 0:")
print(tensor)

print("\n================= Multiple conditions ==============\n")
# Multiple conditions
tensor[(tensor > 1) & (tensor < 4)] = 0
print("After multiple conditions:")
print(tensor)
```

**Expected output**

```
============ Modify elements based on condition ==========

After setting values > 5 to 0:
tensor([[   1,    2,    3],
        [   4,    5, -100],
        [-100, -100, -100]])

================= Multiple conditions ==============

After multiple conditions:
tensor([[   1,    0,    0],
        [   4,    5, -100],
        [-100, -100, -100]])
```

5. **In-place Operations**

```python
tensor = torch.tensor([[1.0, 2.0], 
                       [3.0, 4.0]])

# In-place arithmetic operations
tensor.add_(5)           # Add 5 to all elements (in-place)
print("After add_(5):")
print(tensor)

tensor.mul_(2)           # Multiply all elements by 2 (in-place)
print("\n After mul_(2):")
print(tensor)

# Other in-place operations
tensor.sub_(1)                      # Subtract 1
print("\n After sub_(1): \n", tensor)
tensor.div_(2)                      # Divide by 2
print("\n After div_(2): \n", tensor)
tensor.pow_(2)                      # Square all elements
print("\n After pow_(2): \n", tensor)
```

**Expected output**

```
After add_(5):
tensor([[6., 7.],
        [8., 9.]])

 After mul_(2):
tensor([[12., 14.],
        [16., 18.]])

 After sub_(1): 
 tensor([[11., 13.],
        [15., 17.]])

 After div_(2): 
 tensor([[5.5000, 6.5000],
        [7.5000, 8.5000]])

 After pow_(2): 
 tensor([[30.2500, 42.2500],
        [56.2500, 72.2500]])
```

6. **Using torch.where() for Conditional Modification**

```python
tensor = torch.tensor([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9]])

# Replace values based on condition
modified = torch.where(tensor > 5, 
                      torch.tensor(-10),  # Values where condition is True
                      tensor)            # Values where condition is False
print("Using torch.where():")
print(modified)
```

**Expected output**

```
Using torch.where():
tensor([[  1,   2,   3],
        [  4,   5, -10],
        [-10, -10, -10]])
```

7. **Mathematical Function Modifications** 

```python
tensor = torch.tensor([[1.0, 4.0],
                       [9.0, 16.0]])

# Apply mathematical functions (create new tensors)
sqrt_tensor = torch.sqrt(tensor)          # Square root
log_tensor = torch.log(tensor)            # Natural log
exp_tensor = torch.exp(tensor)            # Exponential

print("\n Square root:\n", sqrt_tensor)
print("\n Log:\n", log_tensor)
print("\n Exponential:\n", exp_tensor)
```

**Expected output**

```
Square root:
 tensor([[1., 2.],
        [3., 4.]])

 Log:
 tensor([[0.0000, 1.3863],
        [2.1972, 2.7726]])

 Exponential:
 tensor([[2.7183e+00, 5.4598e+01],
        [8.1031e+03, 8.8861e+06]])
```

8. **Filling Methods**

 ```python
tensor = torch.tensor([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9],
                       [10, 11, 12]])

# Fill with specific value
tensor.fill_(-5)
print("After fill_(0):")
print(tensor)

# Fill diagonal
tensor.fill_diagonal_(1)
print("\n After fill_diagonal_(1):")
print(tensor)

# Zero out specific elements
tensor.zero_()  # Fill entire tensor with zeros
print("\n After zero_():")
print(tensor)
```

**Expected output**

```
After fill_(0):
tensor([[-5, -5, -5],
        [-5, -5, -5],
        [-5, -5, -5],
        [-5, -5, -5]])

 After fill_diagonal_(1):
tensor([[ 1, -5, -5],
        [-5,  1, -5],
        [-5, -5,  1],
        [-5, -5, -5]])

 After zero_():
tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
```

9. **zeros() and ones() tensors**

```python
tensor = torch.zeros(3,4)
print("zeros(3,4)\n", tensor)

tensor = torch.ones(3,4)
print("\nones(3,4)\n", tensor)
```

**Expected output**

```
zeros(3,4)
 tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])

ones(3,4)
 tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
```
10. **Memory Considerations (In place and outplace modification)**

```python
# In-place vs out-of-place operations
tensor = torch.tensor([1, 2, 3])

# Out-of-place: creates new tensor (more memory)
new_tensor = tensor + 1

# In-place: modifies existing tensor (less memory)
tensor.add_(1)

print("Same memory?", tensor.data_ptr() == new_tensor.data_ptr())  # False
```

**Expected output**

```
Same memory? False
```

## Converting Between NumPy and PyTorch
PyTorch makes it very easy to convert between NumPy arrays and PyTorch tensors. 
1. **NumPy → PyTorch Tensor**

**Shared Memory (from_numpy and as_tensor)**
     
```python
# NumPy to PyTorch with shared memory
numpy_arr = np.array([1, 2, 3, 4, 5])
torch_tensor = torch.from_numpy(numpy_arr)  # Shares memory

print("Original NumPy:", numpy_arr)
print("PyTorch tensor:", torch_tensor)

# Modify NumPy array
numpy_arr[0] = 99
print("\nAfter modifying NumPy:")
print("NumPy array:", numpy_arr)
print("PyTorch tensor:", torch_tensor)  # Also changed!

# Modify PyTorch tensor
torch_tensor[1] = 88
print("\nAfter modifying PyTorch:")
print("NumPy array:", numpy_arr)        # Also changed!
print("PyTorch tensor:", torch_tensor)
```

**Expected output**

```
Original NumPy: [1 2 3 4 5]
PyTorch tensor: tensor([1, 2, 3, 4, 5])

After modifying NumPy:
NumPy array: [99  2  3  4  5]
PyTorch tensor: tensor([99,  2,  3,  4,  5])

After modifying PyTorch:
NumPy array: [99 88  3  4  5]
PyTorch tensor: tensor([99, 88,  3,  4,  5])
```
**Separate Memory (torch.tensor)**

```python
# NumPy to PyTorch with separate memory
numpy_arr = np.array([1, 2, 3, 4, 5])
torch_tensor = torch.tensor(numpy_arr)  # Creates copy

print("Original NumPy:", numpy_arr)
print("PyTorch tensor:", torch_tensor)

# Modify NumPy array
numpy_arr[0] = 99
print("\nAfter modifying NumPy:")
print("NumPy array:", numpy_arr)
print("PyTorch tensor:", torch_tensor)  # Unchanged!

# Modify PyTorch tensor
torch_tensor[1] = 88
print("\nAfter modifying PyTorch:")
print("NumPy array:", numpy_arr)        # Unchanged!
print("PyTorch tensor:", torch_tensor)
```

**Expected output**

```
Original NumPy: [1 2 3 4 5]
PyTorch tensor: tensor([1, 2, 3, 4, 5])

After modifying NumPy:
NumPy array: [99  2  3  4  5]
PyTorch tensor: tensor([1, 2, 3, 4, 5])

After modifying PyTorch:
NumPy array: [99  2  3  4  5]
PyTorch tensor: tensor([ 1, 88,  3,  4,  5])
```

2. **PyTorch → NumPy**

```python
# Create a PyTorch tensor
torch_tensor = torch.tensor([[1.0, 2.0, 3.0],
                             [4.0, 5.0, 6.0]])
print("PyTorch tensor:")
print(torch_tensor)
print("Type:", type(torch_tensor))

# Convert to NumPy array
numpy_array = torch_tensor.numpy()
print("\nNumPy array:")
print(numpy_array)
print("Type:", type(numpy_array))
print("Shape:", numpy_array.shape)
print("Dtype:", numpy_array.dtype)
```

**Expected output**

```
PyTorch tensor:
tensor([[1., 2., 3.],
        [4., 5., 6.]])
Type: <class 'torch.Tensor'>

NumPy array:
[[1. 2. 3.]
 [4. 5. 6.]]
Type: <class 'numpy.ndarray'>
Shape: (2, 3)
Dtype: float32
```

## Changing Tensor Shape in PyTorch
PyTorch provides several methods to reshape tensors. 

1. **PyTorch provides several methods to reshape tensors - Using .reshape()**

```python
import torch

# Original tensor: 3x4
original = torch.tensor([[1, 2, 3, 4],
                         [5, 6, 7, 8],
                         [9, 10, 11, 12]])
print("Original shape (3x4):", original.shape)
print(original)

# Reshape to 2x3x2
reshaped = original.reshape(2, 3, 2)
print("\nReshaped to (2x3x2):", reshaped.shape)
print(reshaped)
```

**Expected output**

```
Original shape (3x4): torch.Size([3, 4])
tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])

Reshaped to (2x3x2): torch.Size([2, 3, 2])
tensor([[[ 1,  2],
         [ 3,  4],
         [ 5,  6]],

        [[ 7,  8],
         [ 9, 10],
         [11, 12]]])
```

2. **PyTorch provides several methods to reshape tensors - Using .view()**

```python
# Using view() - requires contiguous memory
reshaped_view = original.view(2, 3, 2)
print("Using view() - same result:")
print(reshaped_view.shape)
print(reshaped_view)
```

**Expected output**

```
Using view() - same result:
torch.Size([2, 3, 2])
tensor([[[ 1,  2],
         [ 3,  4],
         [ 5,  6]],

        [[ 7,  8],
         [ 9, 10],
         [11, 12]]])
```

3. **Flattening Tensors**

```python
# Flatten to 1D
flattened = original.reshape(-1)      # or original.view(-1)
print("Flattened:", flattened.shape)  # torch.Size([12])
print(flattened)  # tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])

# Flatten specific dimensions
flattened_2d = original.reshape(3, -1)      # Keep first dim, flatten rest
print("Flattened 2D:", flattened_2d.shape)  # torch.Size([3, 4])
print(flattened_2d)
```

**Expected output**

```
Flattened: torch.Size([12])
tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])
Flattened 2D: torch.Size([3, 4])
tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])
```

4. **Adding/Removing Dimensions - unsqueeze(0)/squeeze(0)**

```python
print("Original tensor's dim:", original.shape)  # torch.Size([3, 4])
print(original)
# Add new dimensions
with_batch = original.reshape(1, 3, 4)      # Add batch dimension
print("\nWith batch dim:", with_batch.shape)  # torch.Size([1, 3, 4])

# Using unsqueeze()
with_batch2 = original.unsqueeze(0)           # Add dimension at position 0
print("Using unsqueeze:", with_batch2.shape)  # torch.Size([1, 3, 4])
print(with_batch2)

# Remove dimensions of size 1
squeezed = with_batch2.squeeze(0)        # Remove dimension at position 0
print("\nAfter squeeze:", squeezed.shape)  # torch.Size([3, 4])
print("\nAfter squeeze:\n", squeezed)  
```

**Expected output**

```
Original tensor's dim: torch.Size([3, 4])
tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])

With batch dim: torch.Size([1, 3, 4])
Using unsqueeze: torch.Size([1, 3, 4])
tensor([[[ 1,  2,  3,  4],
         [ 5,  6,  7,  8],
         [ 9, 10, 11, 12]]])

After squeeze: torch.Size([3, 4])

After squeeze:
 tensor([[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]])
```
5. **Automatic Dimension Calculation**

```python
# Let PyTorch calculate one dimension
original = torch.tensor([[1, 2, 3, 4],
                         [5, 6, 7, 8],
                         [9, 10, 11, 12]])

# Use -1 for automatic dimension calculation
auto1 = original.reshape(2, -1)    # 2x6
auto2 = original.reshape(-1, 6)    # 2x6
auto3 = original.reshape(2, 3, -1) # 2x3x2

print("2 x -1:", auto1.shape)      # torch.Size([2, 6])
print("-1 x 6:", auto2.shape)      # torch.Size([2, 6])
print("2 x 3 x -1:", auto3.shape)  # torch.Size([2, 3, 2])
```

**Expected output**

```
2 x -1: torch.Size([2, 6])
-1 x 6: torch.Size([2, 6])
2 x 3 x -1: torch.Size([2, 3, 2])
```
6. **Basic Stacking - torch.stack()**

```python
import torch

# Create individual tensors
tensor1 = torch.tensor([1, 2, 3])
tensor2 = torch.tensor([4, 5, 6])
tensor3 = torch.tensor([7, 8, 9])

print("Individual tensors:")
print("tensor1:", tensor1)
print("tensor2:", tensor2)
print("tensor3:", tensor3)

# Stack along a new dimension (default dim=0)
stacked = torch.stack([tensor1, tensor2, tensor3])
print("\nStacked (dim=0):")
print(stacked)
print("\nShape:", stacked.shape)  # torch.Size([3, 3])
```

**Expected output**

```
Individual tensors:
tensor1: tensor([1, 2, 3])
tensor2: tensor([4, 5, 6])
tensor3: tensor([7, 8, 9])

Stacked (dim=0):
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])

Shape: torch.Size([3, 3])
```

7. **Concatenation (Different from Stacking)**

```python
# Understanding the difference between stack and cat
tensor1 = torch.tensor([1, 2, 3])
tensor2 = torch.tensor([4, 5, 6])

# Concatenate (joins along existing dimension)
concatenated = torch.cat([tensor1, tensor2])
print("Concatenated:", concatenated)              # tensor([1, 2, 3, 4, 5, 6])
print("Concatenated shape:", concatenated.shape)  # torch.Size([6])

# Stack (creates new dimension)
stacked = torch.stack([tensor1, tensor2])
print("\nStacked:\n", stacked)
print("\nStacked shape:", stacked.shape)    # torch.Size([2, 3])
```

**Expected output**

```
Concatenated: tensor([1, 2, 3, 4, 5, 6])
Concatenated shape: torch.Size([6])

Stacked:
 tensor([[1, 2, 3],
        [4, 5, 6]])

Stacked shape: torch.Size([2, 3])
```

