# Convolutional Neural Networks - CNNs
Neural networks can be **fully connected** or **partially connected**. When a neural network is not fully connected, it is called a Convolutional Neural Network (CNN).In a **fully connected network**, every neuron in one layer is connected to **all neurons** in the previous layer. In contrast, in a **convolutional network**, each neuron in a layer is connected only to a **small region** of the neurons in the previous layer. This local connectivity allows CNNs to efficiently detect patterns such as edges, textures, and shapes, which is why **convolutional neural networks are widely used for image processing tasks**.

### Understanding Fully Connected vs. Convolutional Neural Networks

1. **Fully Connected Neural Networks (FCNs)**
   In a fully connected neural network, also called a dense network, every neuron in one layer is connected to all neurons in the next layer. However, this structure becomes **inefficient for large inputs, such as images**, where each pixel would require its own connection. For example, an image of size 100×100 pixels (10,000 values) connected to just one layer of 1,000 neurons would require 10 million connections. Example use: simple classification

2. **Convolutional Neural Networks (CNNs)**
   To solve this problem, Convolutional Neural Networks (CNNs) were introduced. Instead of connecting each neuron to all inputs, CNNs connect each neuron only to a **small local region** of the input (called the **receptive field**). This means:
   * Each neuron processes only part of the input.
   * CNNs can detect **local features** such as edges, corners, and textures.
   * These local features are then combined in deeper layers to recognize **complex patterns** like objects or faces.

**CNN are useful for image processing and vision tasks.**   

  <img width="593" height="231" alt="image" src="https://github.com/user-attachments/assets/bfbaaf9d-0c9d-47db-b10c-79f063acc52a" />

## Image Representation in Neural Networks
Before training a Convolutional Neural Network (CNN), it’s essential to understand how images are represented numerically. Since computers don’t “see” pictures like humans do. They only process numbers.

1. **Images as Matrixes**
   
   Every digital image is made up of **pixels**, and each pixel holds a numerical value that represents its **intensity or color**. A **grayscale image** stores one value per pixel (brightness), usually between **0 and 255** (0 = black, 255 = white, and values in between = shades of gray)
   ```python
   [ [  0,  10,  50, ... ],
     [100, 120,  80, ... ],
     ...
   ]

   ```

2. **Images as 3D Tensors**

   When using deep learning frameworks like PyTorch or TensorFlow, images are represented as tensors (multi-dimensional arrays).
   * **1 channel**: A grayscale image → $28 × 28 × 1$
   * **3 channel**: A color (RGB) image → $28 × 28 × 3$

   **Here:**
     * The first two dimensions represent **height** and **width** (the spatial size).
     * The third dimension represents the **number of color channels**.

| Image Type | Representation | Description                                 |
| ---------- | -------------- | ------------------------------------------- |
| Grayscale  | (28, 28, 1)    | Each pixel has one intensity value          |
| RGB        | (28, 28, 3)    | Each pixel has three color values (R, G, B) |


**CNNs** receive images as **3D tensors** where each channel contains a separate map of pixel intensities.

## Convolution Layer
The convolution layer is the most important part of a Convolutional Neural Network (CNN).It is responsible for automatically learning and detecting important features from the input data (such as edges, textures, and patterns in an image). Instead of connecting every neuron to all inputs (like in fully connected layers), a convolution layer uses **filters** (also called **kernels**) that scan small regions of the input step by step.

### filters (also called kernels)
A **filter** is a small matrix (for example, $3×3$ or $5×5$) that slides over the input image. At each position, the filter and the part of the image it overlaps are multiplied element-by-element, and the results are summed to produce a single number in the output feature map. This process is called **convolution**. Example:

<img width="382" height="227" alt="image" src="https://github.com/user-attachments/assets/f8e08abc-26af-42c5-83bb-1285a51fecd3" />

**$(1×1+0×1+1×1)+(0×0+1×1+0×1)+(1×0+0×0+1×1)=4$**




