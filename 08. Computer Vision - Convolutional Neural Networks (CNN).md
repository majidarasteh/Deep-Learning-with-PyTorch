# Convolutional Neural Networks - CNNs
Neural networks can be **fully connected** or **partially connected**. When a neural network is not fully connected, it is called a Convolutional Neural Network (CNN).In a **fully connected network**, every neuron in one layer is connected to **all neurons** in the previous layer. In contrast, in a **convolutional network**, each neuron in a layer is connected only to a **small region** of the neurons in the previous layer. This local connectivity allows CNNs to efficiently detect patterns such as edges, textures, and shapes, which is why **convolutional neural networks are widely used for image processing tasks**.

### Understanding Fully Connected vs. Convolutional Neural Networks

1. **Fully Connected Neural Networks (FCNs)**
   In a fully connected neural network, also called a dense network, every neuron in one layer is connected to all neurons in the next layer. However, this structure becomes **inefficient for large inputs, such as images**, where each pixel would require its own connection. For example, an image of size 100×100 pixels (10,000 values) connected to just one layer of 1,000 neurons would require 10 million connections. Example use: simple classification

2. **Convolutional Neural Networks (CNNs)**
   To solve this problem, Convolutional Neural Networks (CNNs) were introduced. Instead of connecting each neuron to all inputs, CNNs connect each neuron only to a **small local region** of the input (called the **receptive field**). This means:
   * Each neuron processes only part of the input.
   * CNNs can detect **local features** such as edges, corners, and textures.
   * These local features are then combined in deeper layers to recognize **complex patterns** like objects or faces.

**CNN are useful for image processing and vision tasks.**   

  <img width="593" height="231" alt="image" src="https://github.com/user-attachments/assets/bfbaaf9d-0c9d-47db-b10c-79f063acc52a" />

## Image Representation in Neural Networks
Before training a Convolutional Neural Network (CNN), it’s essential to understand how images are represented numerically. Since computers don’t “see” pictures like humans do. They only process numbers.

1. **Images as Matrixes**
   
   Every digital image is made up of **pixels**, and each pixel holds a numerical value that represents its **intensity or color**. A **grayscale image** stores one value per pixel (brightness), usually between **0 and 255** (0 = black, 255 = white, and values in between = shades of gray)
   ```python
   [ [  0,  10,  50, ... ],
     [100, 120,  80, ... ],
     ...
   ]

   ```

2. **Images as 3D Tensors**

   When using deep learning frameworks like PyTorch or TensorFlow, images are represented as tensors (multi-dimensional arrays).
   * **1 channel**: A grayscale image → $28 × 28 × 1$
   * **3 channel**: A color (RGB) image → $28 × 28 × 3$

   **Here:**
     * The first two dimensions represent **height** and **width** (the spatial size).
     * The third dimension represents the **number of color channels**.

| Image Type | Representation | Description                                 |
| ---------- | -------------- | ------------------------------------------- |
| Grayscale  | (28, 28, 1)    | Each pixel has one intensity value          |
| RGB        | (28, 28, 3)    | Each pixel has three color values (R, G, B) |


**CNNs** receive images as **3D tensors** where each channel contains a separate map of pixel intensities.

## Filter (Kernel) in Convolution Layer
The convolution layer is the most important part of a Convolutional Neural Network (CNN).It is responsible for automatically learning and detecting important features from the input data (such as edges, textures, and patterns in an image). Instead of connecting every neuron to all inputs (like in fully connected layers), a convolution layer uses **filters** (also called **kernels**) that scan small regions of the input step by step.

A **filter (also called kernels)** is a small matrix (for example, $3×3$ or $5×5$) that slides over the input image. At each position, the filter and the part of the image it overlaps are multiplied element-by-element, and the results are summed to produce a single number in the output feature map. This process is called **convolution**. Example:

<img width="382" height="227" alt="image" src="https://github.com/user-attachments/assets/f8e08abc-26af-42c5-83bb-1285a51fecd3" />

**$(1×1+0×1+1×1)+(0×0+1×1+0×1)+(1×0+0×0+1×1)=4$**

After the convolution operation is applied to an image using a filter (kernel), the result is a new matrix called the **feature map** (or **activation map**). The feature map:
1. Is **smaller** than the original image.
2. Contains the **important features** detected by the filter (such as edges or textures),
3. Represents how strongly each region of the image matches the pattern learned by that filter.

Convolutional Neural Networks (**CNNs**) are **not limited to a single convolution layer**. In fact, most CNN architectures consist of **multiple stacked convolutional layers**, where each layer learns a different level of features from the input image.

The **first convolution layer** usually captures low-level features — the basic building blocks of an image, such as:
* **Edges** (horizontal, vertical, or diagonal lines.
* **Colors** and simple textures.

As more layers are added, each new convolution layer receives the feature maps from the previous one as input. This allows deeper layers to learn **higher-level and more abstract** features, such as:
* Corners, curves, and object parts.
* Shapes like eyes, wheels, or faces.
* Entire objects such as animals, vehicles, or human faces.

Each layer builds upon the features detected by the earlier layers, forming a **feature hierarchy** from simple to complex.

## Pooling Layer in Convolutional Neural Networks

A **pooling layer** is a component of a CNN that **reduces the spatial size** of the feature maps while keeping the most important information. Pooling helps:
1. Reduce the** number of parameters** and computation.
2. Prevent **overfitting**.
3. Make the network more **robust** to small translations or distortions in the input.

Pooling does **not learn new weights**, it simply performs a fixed operation on small regions of the input feature map. Pooling works by dividing the input feature map into small, **non-overlapping regions (windows)**, and then computing a summary statistic for each region. The most common types of pooling are:
1. **Max Pooling**: takes the maximum value in the region.
   * Example: Detects the **strongest feature** in that patch.
2. **Average Pooling**: takes the average value of the region.
   * Example: Provides a **smoothed version** of the features.

**Example in pytorch**

```python
   import torch
   import torch.nn as nn
   
   # Example: a grayscale image (1 channel)
   conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)
   
   # Random input image: 1 batch, 1 channel, 5x5 size
   image = torch.randn(1, 1, 5, 5)
   
   # Apply convolution
   output = conv_layer(image)
   
   print("Input shape:", image.shape)
   print("Output shape:", output.shape)
```


**Why Pooling Is Important**

1. **Dimensionality Reduction**: Reduces the size of feature maps, speeding up computation.
2. **Feature Emphasis**: Keeps strong activations while discarding less important ones.
3. **Translation Invariance**: Small shifts or distortions in the input image do not drastically change the pooled output.

Pooling layers are usually placed after convolution layers. They compress feature maps without losing critical information.

<img width="372" height="286" alt="image" src="https://github.com/user-attachments/assets/3043826d-1907-454b-9123-2692a6095a14" />

```python
   import torch
   import torch.nn as nn
   
   # Example feature map: 1 channel, 4x4
   feature_map = torch.tensor([[[[1,3,2,4],
                                 [5,6,1,2],
                                 [7,8,9,1],
                                 [4,3,2,0]]]], dtype=torch.float)
   
   # 2x2 Max Pooling
   pool = nn.MaxPool2d(kernel_size=2, stride=2)
   pooled_output = pool(feature_map)
   
   print("Pooled feature map:\n", pooled_output)
```

**Output**

```python
   tensor([[[[6., 4.],
             [9., 2.]]]])

```

## Convolutional and Pooling Layers Together

In a Convolutional Neural Network (CNN), the convolution layer and the pooling layer together form the **i-th layer of the network**. Depending on the complexity of the images, the number of these layers can be increased to capture more low-level details or more complex patterns.

### Depth, Accuracy, and Speed Trade-Off
There is always a trade-off between network depth, accuracy, and computational speed:

* Adding more layers usually **improves overall accuracy**.
* But it also makes the network **slower**, because each layer adds more computations.

To find the optimal design, different depths can be tested to determine the **best point** that achieves the **highest performance**. It is important to note that adding more layers does **not always linearly increase accuracy**. Often, there is a point of **diminishing returns**, meaning:
* Each additional layer contributes **less improvement** to overall accuracy.
* Beyond a certain depth, adding layers may **no longer be efficient**.

Thus, CNN architecture design involves **balancing accuracy, speed, and computational efficiency** to achieve the best practical results. Once the input image has passed through multiple convolution and pooling layers and has been transformed into a **multi-level feature representation**, it can now be **flattened** into a **single column vector**. This flattened output is then fed into a fully connected (dense) neural network. Following pictures shows an example:

<img width="623" height="209" alt="image" src="https://github.com/user-attachments/assets/a2e8d657-21c0-4d3d-8bf7-d74ad79b7e82" />

During training, **backpropagation** is applied to adjust the network’s weights. After going through several training iterations, the model becomes capable of:
* **Identifying dominant features** of the input image,
* **Distinguishing specific low-level features**, such as edges or textures, that are important for classification.

The following picture illustrates two layers of a Convolutional Neural Network (CNN). After convolutional layers, the feature maps pass through a ReLU activation function. The resulting feature maps are then **flattened into a single column vector**, which is fed into a fully connected (dense) network for further processing and final classification. This process allows the network to combine spatial features learned in the convolutional layers and make predictions based on the most important patterns detected in the input image.

<img width="891" height="445" alt="image" src="https://github.com/user-attachments/assets/4e2b0114-13d5-434d-84ca-347245f826ac" />

## Padding in Convolutional Neural Networks
Sometimes, before applying a filter, it is necessary to add a border (padding) around the input matrix. This ensures that the convolution operation can cover the edges of the image and helps preserve the spatial size of the feature maps. For example, consider a **5×5 input matrix**. By adding a **padding of size 1**, the matrix **becomes 7×7**, with an extra row and column added around all sides. The newly added cells are typically filled with **zeros**, which is why this technique is called **zero-padding**. Zero-padding helps in:
* Preventing the feature **map from shrinking after** each convolution.
* Allowing the filter to access the **edge pixels** of the image.
* Maintaining spatial dimensions when required for deeper layers.

<img width="402" height="163" alt="image" src="https://github.com/user-attachments/assets/a0bda6e9-2f95-4464-aa7f-102b00c8e387" />

## Stride in Convolution and Pooling
In convolution and pooling operations, there is a concept called **stride**. The stride defines **how many steps** the filter (or pooling window) moves forward after each operation. For example, if the stride is set to s, the filter shifts s pixels at a time as it scans across the input matrix.
* **Stride = 1**: The filter moves one step at a time, producing a larger output feature map.
* **Stride > 1**: The filter moves multiple step at a time, producing a smaller output feature map.

Stride allows control over the spatial size of the feature maps and affects the **computational efficiency** of the network.

<img width="624" height="96" alt="image" src="https://github.com/user-attachments/assets/9c593c9b-c13e-4163-9ea7-5e3bed9f1f4e" />

