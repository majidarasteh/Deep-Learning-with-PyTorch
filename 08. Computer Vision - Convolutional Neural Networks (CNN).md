# Computer vision
Computer vision helps machines understand and interpret visual information like humans do. Computer vision is used in many real-world applications, such as:
* **Autonomous vehicles** – detecting lanes, pedestrians, and traffic signs.
* **Medical imaging** – identifying tumors or abnormalities in X-rays and MRIs.
* **Facial recognition** – used in security systems and smartphones.
* **Industrial automation** – inspecting products for quality control.
* **Agriculture** – monitoring crop health using drone imagery.
* **Retail and marketing** – analyzing customer behavior and inventory.

## Computer vision libraries in PyTorch
PyTorch provides several powerful libraries and tools that make it easy to build and train computer vision models:
1. **`torchvision`**: This is the main library for computer vision in PyTorch. It includes:
   * **Datasets** (like CIFAR-10, MNIST, ImageNet)
   * **Transforms** for image preprocessing and augmentation
   * **Models** (pretrained CNNs such as ResNet, VGG, AlexNet)
   * **Utilities** for loading and visualizing images
2. **`torch.utils.data`**: Helps manage datasets and data loaders, making it easy to feed images into a model in batches during training.
3. **`torch.nn`**: Provides all the building blocks for defining layers like convolution, pooling, activation functions, and fully connected layers.
4. **`torchvision.transforms`**: Offers image transformation tools such as resizing, cropping, flipping, and normalization—essential for data preprocessing and augmentation.

Together, these libraries form a flexible and efficient ecosystem for developing computer vision applications such as image classification, object detection, and segmentation.

✅ **Note**:
The `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` classes in PyTorch are **not limited to computer vision** tasks. They are general-purpose tools designed to handle any type of data, including text, audio, tabular data, and more.


# Convolutional Neural Networks - CNNs

Neural networks can be **fully connected** or **partially connected**. When a neural network is not fully connected, it is called a Convolutional Neural Network (CNN).In a **fully connected network**, every neuron in one layer is connected to **all neurons** in the previous layer. In contrast, in a **convolutional network**, each neuron in a layer is connected only to a **small region** of the neurons in the previous layer. This local connectivity allows CNNs to efficiently detect patterns such as edges, textures, and shapes, which is why **convolutional neural networks are widely used for image processing tasks**.

### Understanding Fully Connected vs. Convolutional Neural Networks

1. **Fully Connected Neural Networks (FCNs)**
   In a fully connected neural network, also called a dense network, every neuron in one layer is connected to all neurons in the next layer. However, this structure becomes **inefficient for large inputs, such as images**, where each pixel would require its own connection. For example, an image of size 100×100 pixels (10,000 values) connected to just one layer of 1,000 neurons would require 10 million connections. Example use: simple classification

2. **Convolutional Neural Networks (CNNs)**
   To solve this problem, Convolutional Neural Networks (CNNs) were introduced. Instead of connecting each neuron to all inputs, CNNs connect each neuron only to a **small local region** of the input (called the **receptive field**). This means:
   * Each neuron processes only part of the input.
   * CNNs can detect **local features** such as edges, corners, and textures.
   * These local features are then combined in deeper layers to recognize **complex patterns** like objects or faces.

**CNN are useful for image processing and vision tasks.**   

  <img width="593" height="231" alt="image" src="https://github.com/user-attachments/assets/bfbaaf9d-0c9d-47db-b10c-79f063acc52a" />

## Image Representation in Neural Networks
Before training a Convolutional Neural Network (CNN), it’s essential to understand how images are represented numerically. Since computers don’t “see” pictures like humans do. They only process numbers.

1. **Images as Matrixes**
   
   Every digital image is made up of **pixels**, and each pixel holds a numerical value that represents its **intensity or color**. A **grayscale image** stores one value per pixel (brightness), usually between **0 and 255** (0 = black, 255 = white, and values in between = shades of gray)
   ```python
   [ [  0,  10,  50, ... ],
     [100, 120,  80, ... ],
     ...
   ]

   ```

2. **Images as 3D Tensors**

   When using deep learning frameworks like PyTorch or TensorFlow, images are represented as tensors (multi-dimensional arrays).
   * **1 channel**: A grayscale image → $28 × 28 × 1$
   * **3 channel**: A color (RGB) image → $28 × 28 × 3$

   **Here:**
     * The first two dimensions represent **height** and **width** (the spatial size).
     * The third dimension represents the **number of color channels**.

| Image Type | Representation | Description                                 |
| ---------- | -------------- | ------------------------------------------- |
| Grayscale  | (28, 28, 1)    | Each pixel has one intensity value          |
| RGB        | (28, 28, 3)    | Each pixel has three color values (R, G, B) |


**CNNs** receive images as **3D tensors** where each channel contains a separate map of pixel intensities.

## Filter (Kernel) in Convolution Layer
The convolution layer is the most important part of a Convolutional Neural Network (CNN).It is responsible for automatically learning and detecting important features from the input data (such as edges, textures, and patterns in an image). Instead of connecting every neuron to all inputs (like in fully connected layers), a convolution layer uses **filters** (also called **kernels**) that scan small regions of the input step by step.

A **filter (also called kernels)** is a small matrix (for example, $3×3$ or $5×5$) that slides over the input image. At each position, the filter and the part of the image it overlaps are multiplied element-by-element, and the results are summed to produce a single number in the output feature map. This process is called **convolution**. Example:

<img width="382" height="227" alt="image" src="https://github.com/user-attachments/assets/f8e08abc-26af-42c5-83bb-1285a51fecd3" />

**$(1×1+0×1+1×1)+(0×0+1×1+0×1)+(1×0+0×0+1×1)=4$**

After the convolution operation is applied to an image using a filter (kernel), the result is a new matrix called the **feature map** (or **activation map**). The feature map:
1. Is **smaller** than the original image.
2. Contains the **important features** detected by the filter (such as edges or textures),
3. Represents how strongly each region of the image matches the pattern learned by that filter.

Convolutional Neural Networks (**CNNs**) are **not limited to a single convolution layer**. In fact, most CNN architectures consist of **multiple stacked convolutional layers**, where each layer learns a different level of features from the input image.

The **first convolution layer** usually captures low-level features — the basic building blocks of an image, such as:
* **Edges** (horizontal, vertical, or diagonal lines.
* **Colors** and simple textures.

As more layers are added, each new convolution layer receives the feature maps from the previous one as input. This allows deeper layers to learn **higher-level and more abstract** features, such as:
* Corners, curves, and object parts.
* Shapes like eyes, wheels, or faces.
* Entire objects such as animals, vehicles, or human faces.

Each layer builds upon the features detected by the earlier layers, forming a **feature hierarchy** from simple to complex.

## Pooling Layer in Convolutional Neural Networks

A **pooling layer** is a component of a CNN that **reduces the spatial size** of the feature maps while keeping the most important information. Pooling helps:
1. Reduce the** number of parameters** and computation.
2. Prevent **overfitting**.
3. Make the network more **robust** to small translations or distortions in the input.

Pooling does **not learn new weights**, it simply performs a fixed operation on small regions of the input feature map. Pooling works by dividing the input feature map into small, **non-overlapping regions (windows)**, and then computing a summary statistic for each region. The most common types of pooling are:
1. **Max Pooling**: takes the maximum value in the region.
   * Example: Detects the **strongest feature** in that patch.
2. **Average Pooling**: takes the average value of the region.
   * Example: Provides a **smoothed version** of the features.

**Example in pytorch**

```python
   import torch
   import torch.nn as nn
   
   # Example: a grayscale image (1 channel)
   conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)
   
   # Random input image: 1 batch, 1 channel, 5x5 size
   image = torch.randn(1, 1, 5, 5)
   
   # Apply convolution
   output = conv_layer(image)
   
   print("Input shape:", image.shape)
   print("Output shape:", output.shape)
```


**Why Pooling Is Important**

1. **Dimensionality Reduction**: Reduces the size of feature maps, speeding up computation.
2. **Feature Emphasis**: Keeps strong activations while discarding less important ones.
3. **Translation Invariance**: Small shifts or distortions in the input image do not drastically change the pooled output.

Pooling layers are usually placed after convolution layers. They compress feature maps without losing critical information.

<img width="372" height="286" alt="image" src="https://github.com/user-attachments/assets/3043826d-1907-454b-9123-2692a6095a14" />

```python
   import torch
   import torch.nn as nn
   
   # Example feature map: 1 channel, 4x4
   feature_map = torch.tensor([[[[1,3,2,4],
                                 [5,6,1,2],
                                 [7,8,9,1],
                                 [4,3,2,0]]]], dtype=torch.float)
   
   # 2x2 Max Pooling
   pool = nn.MaxPool2d(kernel_size=2, stride=2)
   pooled_output = pool(feature_map)
   
   print("Pooled feature map:\n", pooled_output)
```

**Output**

```python
   tensor([[[[6., 4.],
             [9., 2.]]]])

```

## Convolutional and Pooling Layers Together

In a Convolutional Neural Network (CNN), the convolution layer and the pooling layer together form the **i-th layer of the network**. Depending on the complexity of the images, the number of these layers can be increased to capture more low-level details or more complex patterns.

### Depth, Accuracy, and Speed Trade-Off
There is always a trade-off between network depth, accuracy, and computational speed:

* Adding more layers usually **improves overall accuracy**.
* But it also makes the network **slower**, because each layer adds more computations.

To find the optimal design, different depths can be tested to determine the **best point** that achieves the **highest performance**. It is important to note that adding more layers does **not always linearly increase accuracy**. Often, there is a point of **diminishing returns**, meaning:
* Each additional layer contributes **less improvement** to overall accuracy.
* Beyond a certain depth, adding layers may **no longer be efficient**.

Thus, CNN architecture design involves **balancing accuracy, speed, and computational efficiency** to achieve the best practical results. Once the input image has passed through multiple convolution and pooling layers and has been transformed into a **multi-level feature representation**, it can now be **flattened** into a **single column vector**. This flattened output is then fed into a fully connected (dense) neural network. Following pictures shows an example:

<img width="623" height="209" alt="image" src="https://github.com/user-attachments/assets/a2e8d657-21c0-4d3d-8bf7-d74ad79b7e82" />

During training, **backpropagation** is applied to adjust the network’s weights. After going through several training iterations, the model becomes capable of:
* **Identifying dominant features** of the input image,
* **Distinguishing specific low-level features**, such as edges or textures, that are important for classification.

The following picture illustrates two layers of a Convolutional Neural Network (CNN). After convolutional layers, the feature maps pass through a ReLU activation function. The resulting feature maps are then **flattened into a single column vector**, which is fed into a fully connected (dense) network for further processing and final classification. This process allows the network to combine spatial features learned in the convolutional layers and make predictions based on the most important patterns detected in the input image.

<img width="891" height="445" alt="image" src="https://github.com/user-attachments/assets/4e2b0114-13d5-434d-84ca-347245f826ac" />

## Padding in Convolutional Neural Networks
Sometimes, before applying a filter, it is necessary to add a border (padding) around the input matrix. This ensures that the convolution operation can cover the edges of the image and helps preserve the spatial size of the feature maps. For example, consider a **5×5 input matrix**. By adding a **padding of size 1**, the matrix **becomes 7×7**, with an extra row and column added around all sides. The newly added cells are typically filled with **zeros**, which is why this technique is called **zero-padding**. Zero-padding helps in:
* Preventing the feature **map from shrinking after** each convolution.
* Allowing the filter to access the **edge pixels** of the image.
* Maintaining spatial dimensions when required for deeper layers.

<img width="402" height="163" alt="image" src="https://github.com/user-attachments/assets/a0bda6e9-2f95-4464-aa7f-102b00c8e387" />

## Stride in Convolution and Pooling
In convolution and pooling operations, there is a concept called **stride**. The stride defines **how many steps** the filter (or pooling window) moves forward after each operation. For example, if the stride is set to s, the filter shifts s pixels at a time as it scans across the input matrix.
* **Stride = 1**: The filter moves one step at a time, producing a larger output feature map.
* **Stride > 1**: The filter moves multiple step at a time, producing a smaller output feature map.

Stride allows control over the spatial size of the feature maps and affects the **computational efficiency** of the network.

<img width="624" height="96" alt="image" src="https://github.com/user-attachments/assets/9c593c9b-c13e-4163-9ea7-5e3bed9f1f4e" />

# Implementing a Binary Image Classification Model (e.g. Cats vs Dogs)

1. **Organize Your Dataset**: Create a clear directory structure so PyTorch can automatically label data using folder names:
   
   ```
       data/
    │
    ├── train/
    │   ├── cats/
    │   └── dogs/
    │
    └── test/
        ├── cats/
        └── dogs/
   ```
   
   Each folder (cats, dogs) should contain images of that class.
   
2. **Transform and Augment the Data**: Always resize and convert images to tensors:
   * Always resize and convert images to tensors:
     
     ```python
        transforms.Resize((128, 128))
        transforms.ToTensor()
     ```
   
   * Normalize to `[0,1]` scale (or use ImageNet mean/std if using pretrained models):
  
     ```python
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
     ```
     
   * Apply **data augmentation only to the training** set to improve generalization:

     ```python
        transforms.RandomHorizontalFlip(),
        transforms.TrivialAugmentWide(),
     ```

   * The **test/validation set should not be augmented**. Only resized, converted, and normalized.

3. **Load Data Using `datasets.ImageFolder`**
   
   * PyTorch can automatically assign labels based on folder names:

     ```python
        from torchvision import datasets

        train_data = datasets.ImageFolder(root="data/train", transform=train_transform)
        test_data = datasets.ImageFolder(root="data/test", transform=test_transform)
     ```

   * **Confirm label mapping:**

     ```python
        print(train_data.classes)   # ['cats', 'dogs']
        print(train_data.class_to_idx)  # {'cats': 0, 'dogs': 1}
     ```

   * **Create DataLoaders**: Wrap datasets into DataLoaders to feed the model in batches:

     ```python
        from torch.utils.data import DataLoader

        train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)
        test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)
     ```

     **Tips:**
     
       * Use `shuffle=True` only for training.
       * `num_workers` can be increased for faster loading (depending on CPU cores).

4. **Define the Model**
   * **Convolutional layers** (`nn.Conv2d`): extract spatial features
   * **Activation functions** (`nn.ReLU`): introduce non-linearity
   * **Pooling layers** (`nn.MaxPool2d`): reduce spatial dimensions and computation
   * **Dropout** (`nn.Dropout`): prevent overfitting
   * **Flattening** (`nn.Flatten`): reshape 2D feature maps into 1D for linear layers
   * **Fully connected (linear) layers** (`nn.Linear`): map features to output class
   * **Output layer**: for binary classification, output one logit (no sigmoid here, since `BCEWithLogitsLoss` applies it internally)

   **Example**

   ```python
      nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),
      nn.MaxPool2d(2),
      nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),
      nn.MaxPool2d(2),
      nn.Flatten(),
      nn.Linear(64*32*32, 1)  # assuming 128x128 input images
   ```

6. **Train the Model**
   * Use `BCEWithLogitsLoss` for **binary classification**.
   * Use `Adam` or `SGD` **optimizer**.
   * **Forward pass → compute loss → backward pass → update weights**
   * Always switch between `model.train()` (**training**) and `model.eval()` (**evaluation**) modes.

     ```python
        criterion = nn.BCEWithLogitsLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
     ```

7. **Predict an Arbitrary Image Label**
   * Load and preprocess a single image:
     
     ```python
        image = Image.open("path/to/image.jpg")
        img_tensor = test_transform(image).unsqueeze(0).to(device)
     ```
     
   * Get prediction:

     ```python
        model.eval()
        with torch.no_grad():
            logit = model(img_tensor)
            prob = torch.sigmoid(logit)
            pred = "dog" if prob > 0.5 else "cat"
        print(f"Predicted: {pred} ({prob.item():.2f})")
     ```

8. **Save the model** Save your model after training:

    ```python
        torch.save(model.state_dict(), "cat_dog_model.pth")
     ```

9. **Image Tensor Shapes**
    
    * Original image shape:
      
      `[batch, color_channels, height, width]` → e.g. `[32, 3, 128, 128]`

    * Matplotlib visualization requires:
      
      `[batch, height, width, color_channels]`

      Use `.permute()` to rearrange:

      ```python
        img_permute = img.permute(0, 2, 3, 1)  # shape: [32, 128, 128, 3]
      ```

      Always check shapes before feeding data into the model to avoid dimension errors.
     
10. **Learning Rate Tip**
    * Start with `1e-3 (0.001)` for Adam optimizer — this often works well.
    * If the model doesn’t learn (loss stuck ~0.693) → increase slightly to `1e-2`.
    * If the model oscillates or diverges → reduce to `1e-4`.

    Example:
    ```python
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    ```
    
11. **Batch Size**
    * Common choices: **16, 32, or 64**.
    * Larger batch sizes → faster computation but more GPU memory usage.
    * Use smaller batches (e.g., 16) if running out of memory.

    Example:
    ```python
        batch_size = 32
    ```

12. **Dropout Rate**
    * Dropout prevents overfitting by randomly turning off neurons during training.
    * Typical values:
      * **After convolutional** layers: **0.2 – 0.3**
      * **After fully** connected layers: **0.5**

    Example:
    ```python
        nn.Dropout(0.3)
    ```



